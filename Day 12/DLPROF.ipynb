{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 methods are available - METHOD1 and METHOD2\n",
    "# METHOD2 is preferrable with dockers - AMP is easily available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: nvidia-pyindex in /home/user/.local/lib/python3.6/site-packages (1.0.9)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: nvidia-dlprof in /home/user/.local/lib/python3.6/site-packages (1.8.0)\n",
      "Requirement already satisfied: nvidia-nsys-cli>=2021.3.2.12 in /home/user/.local/lib/python3.6/site-packages (from nvidia-dlprof) (2021.3.2.12)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: nvidia-dlprof[pytorch] in /home/user/.local/lib/python3.6/site-packages (1.8.0)\n",
      "Requirement already satisfied: nvidia-nsys-cli>=2021.3.2.12 in /home/user/.local/lib/python3.6/site-packages (from nvidia-dlprof[pytorch]) (2021.3.2.12)\n",
      "Requirement already satisfied: nvidia-dlprof-pytorch-nvtx in /home/user/.local/lib/python3.6/site-packages (from nvidia-dlprof[pytorch]) (1.8.0)\n",
      "Requirement already satisfied: torch>=1.2.0 in /home/user/.local/lib/python3.6/site-packages (from nvidia-dlprof-pytorch-nvtx->nvidia-dlprof[pytorch]) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /home/user/.local/lib/python3.6/site-packages (from nvidia-dlprof-pytorch-nvtx->nvidia-dlprof[pytorch]) (1.19.5)\n",
      "Requirement already satisfied: dataclasses in /home/user/.local/lib/python3.6/site-packages (from torch>=1.2.0->nvidia-dlprof-pytorch-nvtx->nvidia-dlprof[pytorch]) (0.8)\n",
      "Requirement already satisfied: typing-extensions in /home/user/.local/lib/python3.6/site-packages (from torch>=1.2.0->nvidia-dlprof-pytorch-nvtx->nvidia-dlprof[pytorch]) (4.0.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: nvidia-dlprofviewer in /home/user/.local/lib/python3.6/site-packages (1.8.0)\n",
      "Requirement already satisfied: setuptools in /home/user/.local/lib/python3.6/site-packages (from nvidia-dlprofviewer) (59.6.0)\n",
      "Requirement already satisfied: gunicorn in /home/user/.local/lib/python3.6/site-packages (from nvidia-dlprofviewer) (20.1.0)\n",
      "Requirement already satisfied: whitenoise in /home/user/.local/lib/python3.6/site-packages (from nvidia-dlprofviewer) (5.3.0)\n",
      "Requirement already satisfied: django==3.2.6 in /home/user/.local/lib/python3.6/site-packages (from nvidia-dlprofviewer) (3.2.6)\n",
      "Requirement already satisfied: uvicorn[standard] in /home/user/.local/lib/python3.6/site-packages (from nvidia-dlprofviewer) (0.16.0)\n",
      "Requirement already satisfied: asgiref<4,>=3.3.2 in /home/user/.local/lib/python3.6/site-packages (from django==3.2.6->nvidia-dlprofviewer) (3.4.1)\n",
      "Requirement already satisfied: pytz in /usr/lib/python3/dist-packages (from django==3.2.6->nvidia-dlprofviewer) (2018.3)\n",
      "Requirement already satisfied: sqlparse>=0.2.2 in /home/user/.local/lib/python3.6/site-packages (from django==3.2.6->nvidia-dlprofviewer) (0.4.2)\n",
      "Requirement already satisfied: h11>=0.8 in /home/user/.local/lib/python3.6/site-packages (from uvicorn[standard]->nvidia-dlprofviewer) (0.13.0)\n",
      "Requirement already satisfied: click>=7.0 in /home/user/.local/lib/python3.6/site-packages (from uvicorn[standard]->nvidia-dlprofviewer) (8.0.4)\n",
      "Requirement already satisfied: typing-extensions in /home/user/.local/lib/python3.6/site-packages (from uvicorn[standard]->nvidia-dlprofviewer) (4.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /home/user/.local/lib/python3.6/site-packages (from uvicorn[standard]->nvidia-dlprofviewer) (0.14.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /home/user/.local/lib/python3.6/site-packages (from uvicorn[standard]->nvidia-dlprofviewer) (6.0)\n",
      "Requirement already satisfied: watchgod>=0.6 in /home/user/.local/lib/python3.6/site-packages (from uvicorn[standard]->nvidia-dlprofviewer) (0.7)\n",
      "Requirement already satisfied: httptools<0.4.0,>=0.2.0 in /home/user/.local/lib/python3.6/site-packages (from uvicorn[standard]->nvidia-dlprofviewer) (0.3.0)\n",
      "Requirement already satisfied: websockets>=9.1 in /home/user/.local/lib/python3.6/site-packages (from uvicorn[standard]->nvidia-dlprofviewer) (9.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /home/user/.local/lib/python3.6/site-packages (from uvicorn[standard]->nvidia-dlprofviewer) (0.19.2)\n",
      "Requirement already satisfied: importlib-metadata in /home/user/.local/lib/python3.6/site-packages (from click>=7.0->uvicorn[standard]->nvidia-dlprofviewer) (4.8.3)\n",
      "Requirement already satisfied: dataclasses in /home/user/.local/lib/python3.6/site-packages (from h11>=0.8->uvicorn[standard]->nvidia-dlprofviewer) (0.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/user/.local/lib/python3.6/site-packages (from importlib-metadata->click>=7.0->uvicorn[standard]->nvidia-dlprofviewer) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "# METHOD1\n",
    "\n",
    "# Installing necessary packages\n",
    "# DLPROF version - 1.8.0\n",
    "\n",
    "!pip install nvidia-pyindex \n",
    "# This package adds the address of the NVIDIA Private Python Package Index(repository of Software)to the user's environment\n",
    "# install the NVIDIA PY index\n",
    "\n",
    "!pip install nvidia-dlprof\n",
    "# install just DLProf and the immediate dependencies\n",
    "\n",
    "!pip install nvidia-dlprof[pytorch]\n",
    "# installs the nvidia-pytorch pip package from the NVIDIA PY index and dlprofâ€™s python pip package for pytorch models\n",
    "# !pip install nvidia_dlprof_pytorch_nvtx - Not needed since exist in prev install\n",
    "# Pytorch does not have built in NVTX ranges around operations. \n",
    "# As such, DLProf has to rely on a python pip package called nvidia_dlprof_pytorch_nvtx \n",
    "# The NVIDIA Tools Extension Library (NVTX) - Applications which integrate NVTX can use NVIDIA Nsight VSE to capture and \n",
    "# visualize these events and ranges.\n",
    "\n",
    "!pip install nvidia-dlprofviewer\n",
    "# To view the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DLProf-09:56:55] Creating Nsys Scheduler\n",
      "[DLProf-09:56:55] RUNNING: nsys profile -t cuda,nvtx -s none --show-output=true --force-overwrite=true --export=sqlite -o ./nsys_profile python3 /home/user/1_VIDHYA/DLPROF/DEMO/train.py\n",
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "Collecting data...\n",
      "Initializing NVTX monkey patches\n",
      "Done with NVTX monkey patching\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 1, 28, 28])\n",
      "torch.Size([32])\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n",
      "train loss:  2.3051528930664062\n",
      "train loss:  2.294670820236206\n",
      "train loss:  2.2926628589630127\n",
      "train loss:  2.2615463733673096\n",
      "train loss:  2.247122287750244\n",
      "train loss:  2.2462029457092285\n",
      "train loss:  2.191889524459839\n",
      "train loss:  2.210902452468872\n",
      "train loss:  2.2127091884613037\n",
      "train loss:  2.131589889526367\n",
      "train loss:  2.1333730220794678\n",
      "train loss:  2.069124698638916\n",
      "train loss:  2.086656332015991\n",
      "train loss:  2.0409796237945557\n",
      "train loss:  1.9782028198242188\n",
      "train loss:  2.1214957237243652\n",
      "train loss:  2.0636279582977295\n",
      "train loss:  1.8870689868927002\n",
      "train loss:  1.9154993295669556\n",
      "train loss:  2.0153331756591797\n",
      "train loss:  1.8003747463226318\n",
      "train loss:  1.6630067825317383\n",
      "train loss:  1.67925226688385\n",
      "train loss:  1.7562261819839478\n",
      "train loss:  1.6872714757919312\n",
      "train loss:  1.5160366296768188\n",
      "train loss:  1.4804166555404663\n",
      "train loss:  1.447832703590393\n",
      "train loss:  1.4537653923034668\n",
      "train loss:  1.4129116535186768\n",
      "train loss:  1.325224757194519\n",
      "train loss:  1.2552210092544556\n",
      "train loss:  1.3403894901275635\n",
      "train loss:  1.4944019317626953\n",
      "train loss:  1.242700219154358\n",
      "train loss:  1.3577463626861572\n",
      "train loss:  1.029811978340149\n",
      "train loss:  0.8660095930099487\n",
      "train loss:  1.217286229133606\n",
      "train loss:  1.0318042039871216\n",
      "train loss:  0.9942198395729065\n",
      "train loss:  0.9467225670814514\n",
      "train loss:  0.9948749542236328\n",
      "train loss:  1.0291311740875244\n",
      "train loss:  0.9018155336380005\n",
      "train loss:  0.860457718372345\n",
      "train loss:  0.7653732895851135\n",
      "train loss:  1.0049889087677002\n",
      "train loss:  0.8371158838272095\n",
      "train loss:  0.7992148399353027\n",
      "train loss:  0.763414204120636\n",
      "train loss:  0.8113858103752136\n",
      "train loss:  0.7225855588912964\n",
      "train loss:  0.46916428208351135\n",
      "train loss:  0.6547088623046875\n",
      "train loss:  0.7326027750968933\n",
      "train loss:  0.777737021446228\n",
      "train loss:  0.6023698449134827\n",
      "train loss:  0.7424428462982178\n",
      "train loss:  0.5118435025215149\n",
      "train loss:  0.6416926383972168\n",
      "train loss:  0.6444294452667236\n",
      "train loss:  0.6980005502700806\n",
      "train loss:  0.6955747008323669\n",
      "train loss:  0.5945144295692444\n",
      "train loss:  0.5660990476608276\n",
      "train loss:  0.5392546653747559\n",
      "train loss:  0.37173452973365784\n",
      "train loss:  0.4935579299926758\n",
      "train loss:  0.5459575653076172\n",
      "train loss:  0.37722229957580566\n",
      "train loss:  0.5965113639831543\n",
      "train loss:  0.5859816670417786\n",
      "train loss:  0.4335708022117615\n",
      "train loss:  0.6941425800323486\n",
      "train loss:  0.8126373887062073\n",
      "train loss:  0.5172148942947388\n",
      "train loss:  0.32587844133377075\n",
      "train loss:  0.23869867622852325\n",
      "train loss:  0.5501534938812256\n",
      "train loss:  0.39248040318489075\n",
      "train loss:  0.47219744324684143\n",
      "train loss:  0.4914872944355011\n",
      "train loss:  0.7193354964256287\n",
      "train loss:  0.5483542680740356\n",
      "train loss:  0.4095216989517212\n",
      "train loss:  0.5787385702133179\n",
      "train loss:  0.5663132071495056\n",
      "train loss:  0.46297141909599304\n",
      "train loss:  0.3517846465110779\n",
      "train loss:  0.6040598750114441\n",
      "train loss:  0.45101475715637207\n",
      "train loss:  0.47608882188796997\n",
      "train loss:  0.4655058681964874\n",
      "train loss:  0.6441903710365295\n",
      "train loss:  0.6467173099517822\n",
      "train loss:  0.41728442907333374\n",
      "train loss:  0.3864208161830902\n",
      "train loss:  0.34242570400238037\n",
      "train loss:  0.31512460112571716\n",
      "train loss:  0.45638689398765564\n",
      "train loss:  0.29653796553611755\n",
      "train loss:  0.5539475679397583\n",
      "train loss:  0.36090749502182007\n",
      "train loss:  0.2824035882949829\n",
      "train loss:  0.5449631214141846\n",
      "train loss:  0.4680742919445038\n",
      "train loss:  0.3054637312889099\n",
      "train loss:  0.43283334374427795\n",
      "train loss:  0.4324849247932434\n",
      "train loss:  0.6082800626754761\n",
      "train loss:  0.41474151611328125\n",
      "train loss:  0.19575627148151398\n",
      "train loss:  0.3286859393119812\n",
      "train loss:  0.5030643939971924\n",
      "train loss:  0.5434178113937378\n",
      "train loss:  0.618257999420166\n",
      "train loss:  0.3322819173336029\n",
      "train loss:  0.5078414678573608\n",
      "train loss:  0.5052475333213806\n",
      "train loss:  0.265542596578598\n",
      "train loss:  0.29826703667640686\n",
      "train loss:  0.2049110233783722\n",
      "train loss:  0.2907615303993225\n",
      "train loss:  0.4059135913848877\n",
      "train loss:  0.33828040957450867\n",
      "train loss:  0.449452668428421\n",
      "train loss:  0.5582762956619263\n",
      "train loss:  0.2820664048194885\n",
      "train loss:  0.6791808605194092\n",
      "train loss:  0.606652557849884\n",
      "train loss:  0.2941388487815857\n",
      "train loss:  0.32902345061302185\n",
      "train loss:  0.5037930607795715\n",
      "train loss:  0.29317647218704224\n",
      "train loss:  0.3624509572982788\n",
      "train loss:  0.5021421313285828\n",
      "train loss:  0.3545229136943817\n",
      "train loss:  0.21378514170646667\n",
      "train loss:  0.5033568739891052\n",
      "train loss:  0.6028023362159729\n",
      "train loss:  0.200118288397789\n",
      "train loss:  0.25754207372665405\n",
      "train loss:  0.3422631323337555\n",
      "train loss:  0.395804226398468\n",
      "train loss:  0.5872294902801514\n",
      "train loss:  0.45087090134620667\n",
      "train loss:  0.17527520656585693\n",
      "train loss:  0.3506329655647278\n",
      "train loss:  0.40543466806411743\n",
      "train loss:  0.47919970750808716\n",
      "train loss:  0.3160542845726013\n",
      "train loss:  0.20261718332767487\n",
      "train loss:  0.39086347818374634\n",
      "train loss:  0.6238570809364319\n",
      "train loss:  0.5025779008865356\n",
      "train loss:  0.38411062955856323\n",
      "train loss:  0.3160055875778198\n",
      "train loss:  0.4393256604671478\n",
      "train loss:  0.24285219609737396\n",
      "train loss:  0.5331137180328369\n",
      "train loss:  0.9949132204055786\n",
      "train loss:  0.21261195838451385\n",
      "train loss:  0.25035524368286133\n",
      "train loss:  0.3250758945941925\n",
      "train loss:  0.6430590748786926\n",
      "train loss:  0.4990141987800598\n",
      "train loss:  0.3435797691345215\n",
      "train loss:  0.3209921419620514\n",
      "train loss:  0.3433569669723511\n",
      "train loss:  0.20002628862857819\n",
      "train loss:  0.19617435336112976\n",
      "train loss:  0.2601816654205322\n",
      "train loss:  0.6323885917663574\n",
      "train loss:  0.1400390863418579\n",
      "train loss:  0.5179024338722229\n",
      "train loss:  0.29577142000198364\n",
      "train loss:  0.44494307041168213\n",
      "train loss:  0.3658111095428467\n",
      "train loss:  0.3684299886226654\n",
      "train loss:  0.25118201971054077\n",
      "train loss:  0.29355305433273315\n",
      "train loss:  0.5400662422180176\n",
      "train loss:  0.36089423298835754\n",
      "train loss:  0.46301260590553284\n",
      "train loss:  0.3214153051376343\n",
      "train loss:  0.44423922896385193\n",
      "train loss:  0.0991910994052887\n",
      "train loss:  0.1979265809059143\n",
      "train loss:  0.30660566687583923\n",
      "train loss:  0.39633792638778687\n",
      "train loss:  0.2536887526512146\n",
      "train loss:  0.20835043489933014\n",
      "train loss:  0.19831490516662598\n",
      "train loss:  0.17117470502853394\n",
      "train loss:  0.49606984853744507\n",
      "train loss:  0.35745298862457275\n",
      "train loss:  0.21348680555820465\n",
      "train loss:  0.24081403017044067\n",
      "train loss:  0.23866258561611176\n",
      "train loss:  0.3129848837852478\n",
      "train loss:  0.2380291074514389\n",
      "train loss:  0.6759896874427795\n",
      "train loss:  0.3785385191440582\n",
      "train loss:  0.3326357901096344\n",
      "train loss:  0.18991431593894958\n",
      "train loss:  0.2765158414840698\n",
      "train loss:  0.2708418071269989\n",
      "train loss:  0.39186805486679077\n",
      "train loss:  0.26524510979652405\n",
      "train loss:  0.26131850481033325\n",
      "train loss:  0.228699192404747\n",
      "train loss:  0.411996990442276\n",
      "train loss:  0.4068400263786316\n",
      "train loss:  0.422346293926239\n",
      "train loss:  0.442250519990921\n",
      "train loss:  0.5708550810813904\n",
      "train loss:  0.3210802674293518\n",
      "train loss:  0.4273056089878082\n",
      "train loss:  0.7530093789100647\n",
      "train loss:  0.2855072021484375\n",
      "train loss:  0.12044975906610489\n",
      "train loss:  0.19373828172683716\n",
      "train loss:  0.5030481815338135\n",
      "train loss:  0.28743523359298706\n",
      "train loss:  0.6121982336044312\n",
      "train loss:  0.4680205285549164\n",
      "train loss:  0.5083733201026917\n",
      "train loss:  0.35536715388298035\n",
      "train loss:  0.37386637926101685\n",
      "train loss:  0.316665917634964\n",
      "train loss:  0.11050606518983841\n",
      "train loss:  0.30647629499435425\n",
      "train loss:  0.14211533963680267\n",
      "train loss:  0.32320693135261536\n",
      "train loss:  0.4813775420188904\n",
      "train loss:  0.08922592550516129\n",
      "train loss:  0.5045414566993713\n",
      "train loss:  0.4658537805080414\n",
      "train loss:  0.28091979026794434\n",
      "train loss:  0.15809544920921326\n",
      "train loss:  0.3407256007194519\n",
      "train loss:  0.41135063767433167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  0.15615323185920715\r\n",
      "train loss:  0.4223440885543823\r\n",
      "train loss:  0.5396680235862732\r\n",
      "train loss:  0.6377934217453003\r\n",
      "train loss:  0.3550412952899933\r\n",
      "train loss:  0.32425543665885925\r\n",
      "train loss:  0.36196064949035645\r\n",
      "train loss:  0.3740728199481964\r\n",
      "train loss:  0.16345001757144928\r\n",
      "train loss:  0.2006111443042755\r\n",
      "train loss:  0.8095879554748535\r\n",
      "train loss:  0.24958327412605286\r\n",
      "train loss:  0.19309967756271362\r\n",
      "train loss:  0.6758601069450378\r\n",
      "train loss:  0.4495583474636078\r\n",
      "train loss:  0.35448750853538513\r\n",
      "train loss:  0.44851091504096985\r\n",
      "train loss:  0.45071661472320557\r\n",
      "train loss:  0.18145379424095154\r\n",
      "train loss:  0.18865442276000977\r\n",
      "train loss:  0.5397353172302246\r\n",
      "train loss:  0.6978704929351807\r\n",
      "train loss:  0.3227984607219696\r\n",
      "train loss:  0.18735907971858978\r\n",
      "train loss:  0.32884955406188965\r\n",
      "train loss:  0.16561758518218994\r\n",
      "train loss:  0.4731195569038391\r\n",
      "train loss:  0.38876888155937195\r\n",
      "train loss:  0.8803187608718872\r\n",
      "train loss:  1.4437031745910645\r\n",
      "train loss:  0.34094110131263733\r\n",
      "train loss:  0.5323988199234009\r\n",
      "train loss:  0.3189099133014679\r\n",
      "train loss:  0.5678924918174744\r\n",
      "train loss:  0.5204059481620789\r\n",
      "train loss:  0.6008325815200806\r\n",
      "train loss:  0.23300282657146454\r\n",
      "train loss:  0.21802140772342682\r\n",
      "train loss:  0.4207073152065277\r\n",
      "train loss:  0.11308801174163818\r\n",
      "train loss:  0.2566194534301758\r\n",
      "train loss:  0.669937014579773\r\n",
      "train loss:  0.41742461919784546\r\n",
      "train loss:  0.23864220082759857\r\n",
      "train loss:  0.15478821098804474\r\n",
      "train loss:  0.36969053745269775\r\n",
      "train loss:  0.5684187412261963\r\n",
      "train loss:  0.35849982500076294\r\n",
      "train loss:  0.4075925350189209\r\n",
      "train loss:  0.29924437403678894\r\n",
      "train loss:  0.456846684217453\r\n",
      "train loss:  0.653715968132019\r\n",
      "train loss:  0.2945152819156647\r\n",
      "train loss:  0.3186437785625458\r\n",
      "train loss:  0.26490190625190735\r\n",
      "train loss:  0.4613446593284607\r\n",
      "train loss:  0.4957314133644104\r\n",
      "train loss:  0.23805467784404755\r\n",
      "train loss:  0.3575460612773895\r\n",
      "train loss:  0.2325824797153473\r\n",
      "train loss:  0.11146333068609238\r\n",
      "train loss:  0.26987504959106445\r\n",
      "train loss:  0.6636990904808044\r\n",
      "train loss:  0.22491644322872162\r\n",
      "train loss:  0.06135524809360504\r\n",
      "train loss:  0.3306537866592407\r\n",
      "train loss:  0.08644208312034607\r\n",
      "train loss:  0.1980542540550232\r\n",
      "train loss:  0.31405600905418396\r\n",
      "train loss:  0.1990007758140564\r\n",
      "train loss:  0.6045634150505066\r\n",
      "train loss:  0.15170778334140778\r\n",
      "train loss:  0.2534422278404236\r\n",
      "train loss:  0.2447863668203354\r\n",
      "train loss:  0.3081226944923401\r\n",
      "train loss:  0.45402297377586365\r\n",
      "train loss:  0.44439375400543213\r\n",
      "train loss:  0.2792353332042694\r\n",
      "train loss:  0.31363987922668457\r\n",
      "train loss:  0.20462697744369507\r\n",
      "train loss:  0.16703741252422333\r\n",
      "train loss:  0.16417552530765533\r\n",
      "train loss:  0.18973274528980255\r\n",
      "train loss:  0.45752906799316406\r\n",
      "train loss:  0.3092757761478424\r\n",
      "train loss:  0.3808214068412781\r\n",
      "train loss:  0.055125415325164795\r\n",
      "train loss:  0.23198464512825012\r\n",
      "train loss:  0.19813929498195648\r\n",
      "train loss:  0.3673690855503082\r\n",
      "train loss:  0.30238884687423706\r\n",
      "train loss:  0.1873403787612915\r\n",
      "train loss:  0.6110761761665344\r\n",
      "train loss:  0.4712362289428711\r\n",
      "train loss:  0.11550867557525635\r\n",
      "train loss:  0.16176463663578033\r\n",
      "train loss:  0.2947697937488556\r\n",
      "train loss:  0.24435867369174957\r\n",
      "train loss:  0.2610212564468384\r\n",
      "train loss:  0.1307525932788849\r\n",
      "train loss:  0.35121867060661316\r\n",
      "train loss:  0.41796016693115234\r\n",
      "train loss:  0.08658519387245178\r\n",
      "train loss:  0.20352758467197418\r\n",
      "train loss:  0.20756307244300842\r\n",
      "train loss:  0.18285143375396729\r\n",
      "train loss:  0.10531491786241531\r\n",
      "train loss:  0.434717059135437\r\n",
      "train loss:  0.29130953550338745\r\n",
      "train loss:  0.16904909908771515\r\n",
      "train loss:  0.14561906456947327\r\n",
      "train loss:  0.1540619134902954\r\n",
      "train loss:  0.37596139311790466\r\n",
      "train loss:  0.2083645761013031\r\n",
      "train loss:  0.2682887315750122\r\n",
      "train loss:  0.23092477023601532\r\n",
      "train loss:  0.2536190450191498\r\n",
      "train loss:  0.4133787453174591\r\n",
      "train loss:  0.7557775974273682\r\n",
      "train loss:  0.42664891481399536\r\n",
      "train loss:  0.4980511963367462\r\n",
      "train loss:  0.38808679580688477\r\n",
      "train loss:  0.8572431802749634\r\n",
      "train loss:  0.32567209005355835\r\n",
      "train loss:  0.294283390045166\r\n",
      "train loss:  0.2233143150806427\r\n",
      "train loss:  0.24906215071678162\r\n",
      "train loss:  0.464369535446167\r\n",
      "train loss:  0.23146390914916992\r\n",
      "train loss:  0.0933406800031662\r\n",
      "train loss:  0.2815309464931488\r\n",
      "train loss:  0.20980165898799896\r\n",
      "train loss:  0.2074277549982071\r\n",
      "train loss:  0.3480491042137146\r\n",
      "train loss:  0.38284948468208313\r\n",
      "train loss:  0.28962600231170654\r\n",
      "train loss:  0.09569032490253448\r\n",
      "train loss:  0.16542653739452362\r\n",
      "train loss:  0.42579174041748047\r\n",
      "train loss:  0.2590256631374359\r\n",
      "train loss:  0.4529432952404022\r\n",
      "train loss:  0.4905817210674286\r\n",
      "train loss:  0.26742470264434814\r\n",
      "train loss:  0.4288307726383209\r\n",
      "train loss:  0.12368690222501755\r\n",
      "train loss:  0.2807585895061493\r\n",
      "train loss:  0.3108370304107666\r\n",
      "train loss:  0.5718634724617004\r\n",
      "train loss:  0.4157197177410126\r\n",
      "train loss:  0.49219459295272827\r\n",
      "train loss:  0.7616576552391052\r\n",
      "train loss:  0.6489580869674683\r\n",
      "train loss:  0.8046830892562866\r\n",
      "train loss:  0.6591126322746277\r\n",
      "train loss:  0.11937833577394485\r\n",
      "train loss:  0.3305247128009796\r\n",
      "train loss:  0.42408257722854614\r\n",
      "train loss:  0.22623756527900696\r\n",
      "train loss:  0.2756774127483368\r\n",
      "train loss:  0.3919963538646698\r\n",
      "train loss:  0.21962310373783112\r\n",
      "train loss:  0.3849310278892517\r\n",
      "train loss:  0.3703891336917877\r\n",
      "train loss:  0.31442421674728394\r\n",
      "train loss:  0.4569728970527649\r\n",
      "train loss:  0.45890626311302185\r\n",
      "train loss:  0.29289737343788147\r\n",
      "train loss:  0.6457411646842957\r\n",
      "train loss:  0.1392875462770462\r\n",
      "train loss:  0.4510922133922577\r\n",
      "train loss:  0.050248511135578156\r\n",
      "train loss:  0.39040952920913696\r\n",
      "train loss:  0.1186457946896553\r\n",
      "train loss:  0.27611473202705383\r\n",
      "train loss:  0.3124266564846039\r\n",
      "train loss:  0.3075457811355591\r\n",
      "train loss:  0.23521767556667328\r\n",
      "train loss:  0.23939895629882812\r\n",
      "train loss:  0.10349378734827042\r\n",
      "train loss:  0.4250337779521942\r\n",
      "train loss:  0.2243064045906067\r\n",
      "train loss:  0.08315836638212204\r\n",
      "train loss:  0.11920499801635742\r\n",
      "train loss:  0.44640472531318665\r\n",
      "train loss:  0.39884206652641296\r\n",
      "train loss:  0.46790385246276855\r\n",
      "train loss:  0.5596370100975037\r\n",
      "train loss:  0.15991021692752838\r\n",
      "train loss:  0.20287944376468658\r\n",
      "train loss:  0.5051934719085693\r\n",
      "train loss:  0.15162627398967743\r\n",
      "train loss:  0.32059434056282043\r\n",
      "train loss:  0.2594645917415619\r\n",
      "train loss:  0.6719627380371094\r\n",
      "train loss:  0.4204452335834503\r\n",
      "train loss:  0.20419324934482574\r\n",
      "train loss:  0.5483972430229187\r\n",
      "train loss:  0.23819290101528168\r\n",
      "train loss:  0.26552411913871765\r\n",
      "train loss:  0.29083168506622314\r\n",
      "train loss:  0.3084745705127716\r\n",
      "train loss:  0.21397726237773895\r\n",
      "train loss:  0.2522372603416443\r\n",
      "train loss:  0.429501473903656\r\n",
      "train loss:  0.39276227355003357\r\n",
      "train loss:  0.4998975396156311\r\n",
      "train loss:  0.6097807884216309\r\n",
      "train loss:  0.208024799823761\r\n",
      "train loss:  0.1595887690782547\r\n",
      "train loss:  0.18546518683433533\r\n",
      "train loss:  0.26775193214416504\r\n",
      "train loss:  0.3872045874595642\r\n",
      "train loss:  0.34759536385536194\r\n",
      "train loss:  0.21632380783557892\r\n",
      "train loss:  0.4647216796875\r\n",
      "train loss:  0.3113402724266052\r\n",
      "train loss:  0.4795890748500824\r\n",
      "train loss:  0.4779653549194336\r\n",
      "train loss:  0.6273380517959595\r\n",
      "train loss:  0.46362757682800293\r\n",
      "train loss:  0.4154530167579651\r\n",
      "train loss:  0.35480350255966187\r\n",
      "train loss:  0.20816917717456818\r\n",
      "train loss:  0.23371313512325287\r\n",
      "train loss:  0.18539786338806152\r\n",
      "train loss:  0.27462878823280334\r\n",
      "train loss:  0.3026696741580963\r\n",
      "train loss:  0.3545304536819458\r\n",
      "train loss:  0.15857627987861633\r\n",
      "train loss:  0.6225693821907043\r\n",
      "train loss:  0.3245275020599365\r\n",
      "train loss:  0.22087019681930542\r\n",
      "train loss:  0.2941822409629822\r\n",
      "train loss:  0.311763733625412\r\n",
      "train loss:  0.2630220353603363\r\n",
      "train loss:  0.12394878268241882\r\n",
      "train loss:  0.29438233375549316\r\n",
      "train loss:  0.2083640843629837\r\n",
      "train loss:  0.20303161442279816\r\n",
      "train loss:  0.23794707655906677\r\n",
      "train loss:  0.18983124196529388\r\n",
      "train loss:  0.34826093912124634\r\n",
      "train loss:  0.24997453391551971\r\n",
      "train loss:  0.20862028002738953\r\n",
      "train loss:  0.2044571042060852\r\n",
      "train loss:  0.1540241241455078\r\n",
      "train loss:  0.1547672301530838\r\n",
      "train loss:  0.08171464502811432\r\n",
      "train loss:  0.5499662756919861\r\n",
      "train loss:  0.46632838249206543\r\n",
      "train loss:  0.325582355260849\r\n",
      "train loss:  0.2001148909330368\r\n",
      "train loss:  0.43111947178840637\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  0.3588760495185852\r\n",
      "train loss:  0.14295737445354462\r\n",
      "train loss:  0.5809762477874756\r\n",
      "train loss:  0.3376503586769104\r\n",
      "train loss:  0.31949740648269653\r\n",
      "train loss:  0.7336304187774658\r\n",
      "train loss:  0.1417253166437149\r\n",
      "train loss:  0.2510531544685364\r\n",
      "train loss:  0.17007869482040405\r\n",
      "train loss:  0.11643731594085693\r\n",
      "train loss:  0.3551183044910431\r\n",
      "train loss:  0.03668070212006569\r\n",
      "train loss:  0.142784982919693\r\n",
      "train loss:  0.32724276185035706\r\n",
      "train loss:  0.12074586749076843\r\n",
      "train loss:  0.24010373651981354\r\n",
      "train loss:  0.07558908313512802\r\n",
      "train loss:  0.34898641705513\r\n",
      "train loss:  0.12864947319030762\r\n",
      "train loss:  0.4644426107406616\r\n",
      "train loss:  0.11980772763490677\r\n",
      "train loss:  0.498849481344223\r\n",
      "train loss:  0.19518405199050903\r\n",
      "train loss:  0.2975035309791565\r\n",
      "train loss:  0.3524659276008606\r\n",
      "train loss:  0.45007622241973877\r\n",
      "train loss:  0.18949858844280243\r\n",
      "train loss:  0.32603180408477783\r\n",
      "train loss:  0.3647853434085846\r\n",
      "train loss:  0.14608635008335114\r\n",
      "train loss:  0.2143128663301468\r\n",
      "train loss:  0.31083187460899353\r\n",
      "train loss:  0.24338285624980927\r\n",
      "train loss:  0.2598259449005127\r\n",
      "train loss:  0.26859596371650696\r\n",
      "train loss:  0.3549302816390991\r\n",
      "train loss:  0.2821228802204132\r\n",
      "train loss:  0.16264507174491882\r\n",
      "train loss:  0.2169158011674881\r\n",
      "train loss:  0.2233198583126068\r\n",
      "train loss:  0.19609788060188293\r\n",
      "train loss:  0.23496630787849426\r\n",
      "train loss:  0.5277018547058105\r\n",
      "train loss:  0.11480052769184113\r\n",
      "train loss:  0.2076529562473297\r\n",
      "train loss:  0.05448378995060921\r\n",
      "train loss:  0.09997669607400894\r\n",
      "train loss:  0.2731870114803314\r\n",
      "train loss:  0.2572462558746338\r\n",
      "train loss:  0.11563289165496826\r\n",
      "train loss:  0.5233312249183655\r\n",
      "train loss:  0.15364907681941986\r\n",
      "train loss:  0.5328492522239685\r\n",
      "train loss:  0.4160044193267822\r\n",
      "train loss:  0.11465941369533539\r\n",
      "train loss:  0.17014968395233154\r\n",
      "train loss:  0.2243603616952896\r\n",
      "train loss:  0.4327481985092163\r\n",
      "train loss:  0.24134042859077454\r\n",
      "train loss:  0.2928319275379181\r\n",
      "train loss:  0.5941281318664551\r\n",
      "train loss:  0.1558672934770584\r\n",
      "train loss:  0.2801138162612915\r\n",
      "train loss:  0.2713179588317871\r\n",
      "train loss:  0.28665032982826233\r\n",
      "train loss:  0.18872615694999695\r\n",
      "train loss:  0.32178962230682373\r\n",
      "train loss:  0.1313876360654831\r\n",
      "train loss:  0.1669795662164688\r\n",
      "train loss:  0.24475038051605225\r\n",
      "train loss:  0.08099428564310074\r\n",
      "train loss:  0.18032225966453552\r\n",
      "train loss:  0.04589943587779999\r\n",
      "train loss:  0.190910205245018\r\n",
      "train loss:  0.16411548852920532\r\n",
      "train loss:  0.2363758385181427\r\n",
      "train loss:  0.18343578279018402\r\n",
      "train loss:  0.26362597942352295\r\n",
      "train loss:  0.10187236964702606\r\n",
      "train loss:  0.25311005115509033\r\n",
      "train loss:  0.12099561840295792\r\n",
      "train loss:  0.34228193759918213\r\n",
      "train loss:  0.3001536428928375\r\n",
      "train loss:  0.06791894882917404\r\n",
      "train loss:  0.10616209357976913\r\n",
      "train loss:  0.3774811625480652\r\n",
      "train loss:  0.2688821852207184\r\n",
      "train loss:  0.13680997490882874\r\n",
      "train loss:  0.2627671957015991\r\n",
      "train loss:  0.4506973326206207\r\n",
      "train loss:  0.07382462918758392\r\n",
      "train loss:  0.12498418241739273\r\n",
      "train loss:  0.3756186068058014\r\n",
      "train loss:  0.05491575226187706\r\n",
      "train loss:  0.2064431607723236\r\n",
      "train loss:  0.06630879640579224\r\n",
      "train loss:  0.05974186956882477\r\n",
      "train loss:  0.2453521192073822\r\n",
      "train loss:  0.27344560623168945\r\n",
      "train loss:  0.1996295303106308\r\n",
      "train loss:  0.271132230758667\r\n",
      "train loss:  0.29556283354759216\r\n",
      "train loss:  0.23215903341770172\r\n",
      "train loss:  0.08245526999235153\r\n",
      "train loss:  0.17172127962112427\r\n",
      "train loss:  0.40797343850135803\r\n",
      "train loss:  0.15494555234909058\r\n",
      "train loss:  0.08016500622034073\r\n",
      "train loss:  0.3170650899410248\r\n",
      "train loss:  0.2867389917373657\r\n",
      "train loss:  0.13555684685707092\r\n",
      "train loss:  0.17252720892429352\r\n",
      "train loss:  0.09243528544902802\r\n",
      "train loss:  0.39996927976608276\r\n",
      "train loss:  0.2758999168872833\r\n",
      "train loss:  0.2777055501937866\r\n",
      "train loss:  0.21416355669498444\r\n",
      "train loss:  0.15085731446743011\r\n",
      "train loss:  0.08362825214862823\r\n",
      "train loss:  0.09326745569705963\r\n",
      "train loss:  0.03633235767483711\r\n",
      "train loss:  0.14159682393074036\r\n",
      "train loss:  0.2340700477361679\r\n",
      "train loss:  0.4392138123512268\r\n",
      "train loss:  0.1440104842185974\r\n",
      "train loss:  0.15263709425926208\r\n",
      "train loss:  0.17033343017101288\r\n",
      "train loss:  0.24866251647472382\r\n",
      "train loss:  0.14907009899616241\r\n",
      "train loss:  0.407220721244812\r\n",
      "train loss:  0.21359318494796753\r\n",
      "train loss:  0.3492025136947632\r\n",
      "train loss:  0.20753955841064453\r\n",
      "train loss:  0.08621960133314133\r\n",
      "train loss:  0.8809700012207031\r\n",
      "train loss:  0.2666594088077545\r\n",
      "train loss:  0.20625482499599457\r\n",
      "train loss:  0.13196900486946106\r\n",
      "train loss:  0.09697232395410538\r\n",
      "train loss:  0.24583491683006287\r\n",
      "train loss:  0.08626589179039001\r\n",
      "train loss:  0.1549661010503769\r\n",
      "train loss:  0.11530803889036179\r\n",
      "train loss:  0.1326768398284912\r\n",
      "train loss:  0.06522991508245468\r\n",
      "train loss:  0.22557884454727173\r\n",
      "train loss:  0.4479658305644989\r\n",
      "train loss:  0.2878926396369934\r\n",
      "train loss:  0.10029608756303787\r\n",
      "train loss:  0.29512402415275574\r\n",
      "train loss:  0.4434218406677246\r\n",
      "train loss:  0.4870041012763977\r\n",
      "train loss:  0.2998630702495575\r\n",
      "train loss:  0.4669562578201294\r\n",
      "train loss:  0.20346282422542572\r\n",
      "train loss:  0.2037295699119568\r\n",
      "train loss:  0.3470264971256256\r\n",
      "train loss:  0.5825147032737732\r\n",
      "train loss:  0.532119870185852\r\n",
      "train loss:  0.43215590715408325\r\n",
      "train loss:  0.4007367193698883\r\n",
      "train loss:  0.3160790801048279\r\n",
      "train loss:  0.34665435552597046\r\n",
      "train loss:  0.1947147697210312\r\n",
      "train loss:  0.3615242838859558\r\n",
      "train loss:  0.057503558695316315\r\n",
      "train loss:  0.4141162037849426\r\n",
      "train loss:  0.12550275027751923\r\n",
      "train loss:  0.08236429840326309\r\n",
      "train loss:  0.421355277299881\r\n",
      "train loss:  0.24525971710681915\r\n",
      "train loss:  0.25059810280799866\r\n",
      "train loss:  0.2713545560836792\r\n",
      "train loss:  0.25816282629966736\r\n",
      "train loss:  0.19123098254203796\r\n",
      "train loss:  0.046677179634571075\r\n",
      "train loss:  0.14890873432159424\r\n",
      "train loss:  0.16059377789497375\r\n",
      "train loss:  0.41788244247436523\r\n",
      "train loss:  0.41775062680244446\r\n",
      "train loss:  0.21302960813045502\r\n",
      "train loss:  0.1528085321187973\r\n",
      "train loss:  0.2359142005443573\r\n",
      "train loss:  0.1728343665599823\r\n",
      "train loss:  0.044289447367191315\r\n",
      "train loss:  0.06131744384765625\r\n",
      "train loss:  0.06169316917657852\r\n",
      "train loss:  0.0847482979297638\r\n",
      "train loss:  0.1640724539756775\r\n",
      "train loss:  0.28381073474884033\r\n",
      "train loss:  0.34422093629837036\r\n",
      "train loss:  0.24380648136138916\r\n",
      "train loss:  0.05583188310265541\r\n",
      "train loss:  0.1218501478433609\r\n",
      "train loss:  0.14433401823043823\r\n",
      "train loss:  0.48194682598114014\r\n",
      "train loss:  0.23532256484031677\r\n",
      "train loss:  0.46999213099479675\r\n",
      "train loss:  0.4630623459815979\r\n",
      "train loss:  0.11659271270036697\r\n",
      "train loss:  0.24008652567863464\r\n",
      "train loss:  0.21940048038959503\r\n",
      "train loss:  0.06965459138154984\r\n",
      "train loss:  0.0484379343688488\r\n",
      "train loss:  0.11902432143688202\r\n",
      "train loss:  0.19636259973049164\r\n",
      "train loss:  0.48085686564445496\r\n",
      "train loss:  0.6422048211097717\r\n",
      "train loss:  0.4394417405128479\r\n",
      "train loss:  0.5134080648422241\r\n",
      "train loss:  0.4582298696041107\r\n",
      "train loss:  0.34791338443756104\r\n",
      "train loss:  0.20752617716789246\r\n",
      "train loss:  0.20911116898059845\r\n",
      "train loss:  0.1884535849094391\r\n",
      "train loss:  0.1611565500497818\r\n",
      "train loss:  0.23129698634147644\r\n",
      "train loss:  0.06098942831158638\r\n",
      "train loss:  0.0881168469786644\r\n",
      "train loss:  0.34103837609291077\r\n",
      "train loss:  0.1046132892370224\r\n",
      "train loss:  0.06559781730175018\r\n",
      "train loss:  0.06213881075382233\r\n",
      "train loss:  0.35261890292167664\r\n",
      "train loss:  0.13449256122112274\r\n",
      "train loss:  0.6523606181144714\r\n",
      "train loss:  0.24338243901729584\r\n",
      "train loss:  0.18655671179294586\r\n",
      "train loss:  0.19416743516921997\r\n",
      "train loss:  0.23865452408790588\r\n",
      "train loss:  0.04471779987215996\r\n",
      "train loss:  0.12394548952579498\r\n",
      "train loss:  0.0525984913110733\r\n",
      "train loss:  0.22363944351673126\r\n",
      "train loss:  0.20556481182575226\r\n",
      "train loss:  0.21561771631240845\r\n",
      "train loss:  0.12461122870445251\r\n",
      "train loss:  0.32973894476890564\r\n",
      "train loss:  0.24732305109500885\r\n",
      "train loss:  0.1587158739566803\r\n",
      "train loss:  0.19061332941055298\r\n",
      "train loss:  0.31548213958740234\r\n",
      "train loss:  0.32998427748680115\r\n",
      "train loss:  0.18162696063518524\r\n",
      "train loss:  0.2888675928115845\r\n",
      "train loss:  0.4505070447921753\r\n",
      "train loss:  0.16502387821674347\r\n",
      "train loss:  0.17269842326641083\r\n",
      "train loss:  0.2905740439891815\r\n",
      "train loss:  0.2967560887336731\r\n",
      "train loss:  0.2117677927017212\r\n",
      "train loss:  0.40505921840667725\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  0.3949311673641205\r\n",
      "train loss:  0.2164657711982727\r\n",
      "train loss:  0.17590293288230896\r\n",
      "train loss:  0.3378504812717438\r\n",
      "train loss:  0.08033047616481781\r\n",
      "train loss:  0.19953100383281708\r\n",
      "train loss:  0.07765469700098038\r\n",
      "train loss:  0.16634830832481384\r\n",
      "train loss:  0.3044032156467438\r\n",
      "train loss:  0.13118772208690643\r\n",
      "train loss:  0.4673217535018921\r\n",
      "train loss:  0.14831170439720154\r\n",
      "train loss:  0.11170628666877747\r\n",
      "train loss:  0.09822313487529755\r\n",
      "train loss:  0.18826547265052795\r\n",
      "train loss:  0.2808830440044403\r\n",
      "train loss:  0.17038384079933167\r\n",
      "train loss:  0.21745045483112335\r\n",
      "train loss:  0.426891028881073\r\n",
      "train loss:  0.25442513823509216\r\n",
      "train loss:  0.15427370369434357\r\n",
      "train loss:  0.4105234146118164\r\n",
      "train loss:  0.3187893033027649\r\n",
      "train loss:  0.23639936745166779\r\n",
      "train loss:  0.2628854513168335\r\n",
      "train loss:  0.25371500849723816\r\n",
      "train loss:  0.5345227718353271\r\n",
      "train loss:  0.13175295293331146\r\n",
      "train loss:  0.09849275648593903\r\n",
      "train loss:  0.2022099643945694\r\n",
      "train loss:  0.17303037643432617\r\n",
      "train loss:  0.4153454601764679\r\n",
      "train loss:  0.32125547528266907\r\n",
      "train loss:  0.11968296766281128\r\n",
      "train loss:  0.06857158243656158\r\n",
      "train loss:  0.08314952254295349\r\n",
      "train loss:  0.283940851688385\r\n",
      "train loss:  0.08184422552585602\r\n",
      "train loss:  0.26261886954307556\r\n",
      "train loss:  0.20366230607032776\r\n",
      "train loss:  0.15492793917655945\r\n",
      "train loss:  0.08566676080226898\r\n",
      "train loss:  0.23598456382751465\r\n",
      "train loss:  0.21112467348575592\r\n",
      "train loss:  0.032560426741838455\r\n",
      "train loss:  0.14397381246089935\r\n",
      "train loss:  0.03096691332757473\r\n",
      "train loss:  0.09615229070186615\r\n",
      "train loss:  0.27698537707328796\r\n",
      "train loss:  0.054729629307985306\r\n",
      "train loss:  0.3739602267742157\r\n",
      "train loss:  0.14958153665065765\r\n",
      "train loss:  0.14126378297805786\r\n",
      "train loss:  0.2020016610622406\r\n",
      "train loss:  0.2766127288341522\r\n",
      "train loss:  0.10226895660161972\r\n",
      "train loss:  0.2210351824760437\r\n",
      "train loss:  0.17239883542060852\r\n",
      "train loss:  0.49107080698013306\r\n",
      "train loss:  0.20117750763893127\r\n",
      "train loss:  0.09609618037939072\r\n",
      "train loss:  0.07108239084482193\r\n",
      "train loss:  0.2486991286277771\r\n",
      "train loss:  0.20419439673423767\r\n",
      "train loss:  0.1715531051158905\r\n",
      "train loss:  0.13994047045707703\r\n",
      "train loss:  0.23563189804553986\r\n",
      "train loss:  0.043624136596918106\r\n",
      "train loss:  0.15283796191215515\r\n",
      "train loss:  0.14596357941627502\r\n",
      "train loss:  0.15847985446453094\r\n",
      "train loss:  0.1688011884689331\r\n",
      "train loss:  0.23672017455101013\r\n",
      "train loss:  0.20839239656925201\r\n",
      "train loss:  0.056530486792325974\r\n",
      "train loss:  0.2978009879589081\r\n",
      "train loss:  0.4772006869316101\r\n",
      "train loss:  0.30071529746055603\r\n",
      "train loss:  0.2192038893699646\r\n",
      "train loss:  0.20342926681041718\r\n",
      "train loss:  0.4569183588027954\r\n",
      "train loss:  0.14045122265815735\r\n",
      "train loss:  0.3118823766708374\r\n",
      "train loss:  0.14346057176589966\r\n",
      "train loss:  0.7495132684707642\r\n",
      "train loss:  0.0679566040635109\r\n",
      "train loss:  0.3322392404079437\r\n",
      "train loss:  0.791320264339447\r\n",
      "train loss:  0.21857072412967682\r\n",
      "train loss:  0.2851409614086151\r\n",
      "train loss:  0.09685512632131577\r\n",
      "train loss:  0.33099308609962463\r\n",
      "train loss:  0.533227264881134\r\n",
      "train loss:  0.09219007939100266\r\n",
      "train loss:  0.05923235043883324\r\n",
      "train loss:  0.11805832386016846\r\n",
      "train loss:  0.12445332854986191\r\n",
      "train loss:  0.045878827571868896\r\n",
      "train loss:  0.19614513218402863\r\n",
      "train loss:  0.2662891149520874\r\n",
      "train loss:  0.48983311653137207\r\n",
      "train loss:  0.5144689083099365\r\n",
      "train loss:  0.2592213749885559\r\n",
      "train loss:  0.2853907346725464\r\n",
      "train loss:  0.0788678228855133\r\n",
      "train loss:  0.15549597144126892\r\n",
      "train loss:  0.11703217029571533\r\n",
      "train loss:  0.07690422981977463\r\n",
      "train loss:  0.12026820331811905\r\n",
      "train loss:  0.17055784165859222\r\n",
      "train loss:  0.16927802562713623\r\n",
      "train loss:  0.22908207774162292\r\n",
      "train loss:  0.20000696182250977\r\n",
      "train loss:  0.09154818207025528\r\n",
      "train loss:  0.16210049390792847\r\n",
      "train loss:  0.28754132986068726\r\n",
      "train loss:  0.24930095672607422\r\n",
      "train loss:  0.22016587853431702\r\n",
      "train loss:  0.3099213242530823\r\n",
      "train loss:  0.08119694888591766\r\n",
      "train loss:  0.2806521952152252\r\n",
      "train loss:  0.1426365077495575\r\n",
      "train loss:  0.23103198409080505\r\n",
      "train loss:  0.12573912739753723\r\n",
      "train loss:  0.10893362015485764\r\n",
      "train loss:  0.15021462738513947\r\n",
      "train loss:  0.11971906572580338\r\n",
      "train loss:  0.12173739075660706\r\n",
      "train loss:  0.03971298784017563\r\n",
      "train loss:  0.14897197484970093\r\n",
      "train loss:  0.1161125972867012\r\n",
      "train loss:  0.27918684482574463\r\n",
      "train loss:  0.12318499386310577\r\n",
      "train loss:  0.18357262015342712\r\n",
      "train loss:  0.09324448555707932\r\n",
      "train loss:  0.177069291472435\r\n",
      "train loss:  0.11916308104991913\r\n",
      "train loss:  0.019161313772201538\r\n",
      "train loss:  0.8546400666236877\r\n",
      "train loss:  0.4033525586128235\r\n",
      "train loss:  0.12587976455688477\r\n",
      "train loss:  0.07841009646654129\r\n",
      "train loss:  0.2863253653049469\r\n",
      "train loss:  0.1669311225414276\r\n",
      "train loss:  0.2492634803056717\r\n",
      "train loss:  0.2061961442232132\r\n",
      "train loss:  0.2871078550815582\r\n",
      "train loss:  0.4951729476451874\r\n",
      "train loss:  0.07386452704668045\r\n",
      "train loss:  0.32547351717948914\r\n",
      "train loss:  0.08831475675106049\r\n",
      "train loss:  0.24583497643470764\r\n",
      "train loss:  0.13917772471904755\r\n",
      "train loss:  0.18673798441886902\r\n",
      "train loss:  0.0661853477358818\r\n",
      "train loss:  0.1340235024690628\r\n",
      "train loss:  0.3092333972454071\r\n",
      "train loss:  0.12806499004364014\r\n",
      "train loss:  0.17364610731601715\r\n",
      "train loss:  0.09378036111593246\r\n",
      "train loss:  0.23274901509284973\r\n",
      "train loss:  0.23363308608531952\r\n",
      "train loss:  0.361751526594162\r\n",
      "train loss:  0.2772817015647888\r\n",
      "train loss:  0.2581857442855835\r\n",
      "train loss:  0.1601668894290924\r\n",
      "train loss:  0.05742117390036583\r\n",
      "train loss:  0.3071175217628479\r\n",
      "train loss:  0.31479892134666443\r\n",
      "train loss:  0.16506272554397583\r\n",
      "train loss:  0.1220649853348732\r\n",
      "train loss:  0.2297259122133255\r\n",
      "train loss:  0.15305021405220032\r\n",
      "train loss:  0.11598750948905945\r\n",
      "train loss:  0.21165908873081207\r\n",
      "train loss:  0.10013314336538315\r\n",
      "train loss:  0.2358267456293106\r\n",
      "train loss:  0.08901100605726242\r\n",
      "train loss:  0.1593514382839203\r\n",
      "train loss:  0.20111742615699768\r\n",
      "train loss:  0.45365825295448303\r\n",
      "train loss:  0.19252987205982208\r\n",
      "train loss:  0.10629048198461533\r\n",
      "train loss:  0.11746598780155182\r\n",
      "train loss:  0.3972620368003845\r\n",
      "train loss:  0.24556995928287506\r\n",
      "train loss:  0.20153674483299255\r\n",
      "train loss:  0.31948021054267883\r\n",
      "train loss:  0.3663019835948944\r\n",
      "train loss:  0.26293274760246277\r\n",
      "train loss:  0.20334848761558533\r\n",
      "train loss:  0.2895072400569916\r\n",
      "train loss:  0.1493586301803589\r\n",
      "train loss:  0.30996161699295044\r\n",
      "train loss:  0.28945156931877136\r\n",
      "train loss:  0.20844095945358276\r\n",
      "train loss:  0.35441717505455017\r\n",
      "train loss:  0.17726655304431915\r\n",
      "train loss:  0.1383720338344574\r\n",
      "train loss:  0.17064711451530457\r\n",
      "train loss:  0.1076996847987175\r\n",
      "train loss:  0.09717653691768646\r\n",
      "train loss:  0.2018708735704422\r\n",
      "train loss:  0.16555693745613098\r\n",
      "train loss:  0.2980203330516815\r\n",
      "train loss:  0.28398704528808594\r\n",
      "train loss:  0.19002926349639893\r\n",
      "train loss:  0.12756741046905518\r\n",
      "train loss:  0.27120622992515564\r\n",
      "train loss:  0.17171400785446167\r\n",
      "train loss:  0.24485436081886292\r\n",
      "train loss:  0.18717293441295624\r\n",
      "train loss:  0.2499723583459854\r\n",
      "train loss:  0.168569877743721\r\n",
      "train loss:  0.12582345306873322\r\n",
      "train loss:  0.26815202832221985\r\n",
      "train loss:  0.19563275575637817\r\n",
      "train loss:  0.479753315448761\r\n",
      "train loss:  0.12322869151830673\r\n",
      "train loss:  0.3360461890697479\r\n",
      "train loss:  0.1789361536502838\r\n",
      "train loss:  0.300638884305954\r\n",
      "train loss:  0.1323944330215454\r\n",
      "train loss:  0.16183719038963318\r\n",
      "train loss:  0.4912444055080414\r\n",
      "train loss:  0.04703159257769585\r\n",
      "train loss:  0.7065749168395996\r\n",
      "train loss:  0.1666063815355301\r\n",
      "train loss:  0.30215853452682495\r\n",
      "train loss:  0.3484576344490051\r\n",
      "train loss:  0.6272924542427063\r\n",
      "train loss:  0.3509892225265503\r\n",
      "train loss:  0.25533679127693176\r\n",
      "train loss:  0.16957156360149384\r\n",
      "train loss:  0.26454341411590576\r\n",
      "train loss:  0.1550825983285904\r\n",
      "train loss:  0.09839266538619995\r\n",
      "train loss:  0.10754529386758804\r\n",
      "train loss:  0.278550386428833\r\n",
      "train loss:  0.4124116897583008\r\n",
      "train loss:  0.07519689202308655\r\n",
      "train loss:  0.522861659526825\r\n",
      "train loss:  0.3444659411907196\r\n",
      "train loss:  0.3653915226459503\r\n",
      "train loss:  0.260942280292511\r\n",
      "train loss:  0.24876292049884796\r\n",
      "train loss:  0.06350817531347275\r\n",
      "train loss:  0.38174426555633545\r\n",
      "train loss:  0.23596888780593872\r\n",
      "train loss:  0.1525699347257614\r\n",
      "train loss:  0.23404234647750854\r\n",
      "train loss:  0.08436044305562973\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  0.5188462138175964\r\n",
      "train loss:  0.22990557551383972\r\n",
      "train loss:  0.10347782075405121\r\n",
      "train loss:  0.2754303216934204\r\n",
      "train loss:  0.20181676745414734\r\n",
      "train loss:  0.41639330983161926\r\n",
      "train loss:  0.21736037731170654\r\n",
      "train loss:  0.2112039029598236\r\n",
      "train loss:  0.3300105035305023\r\n",
      "train loss:  0.2665245234966278\r\n",
      "train loss:  0.5739402174949646\r\n",
      "train loss:  0.2772211730480194\r\n",
      "train loss:  0.18285290896892548\r\n",
      "train loss:  0.5717678070068359\r\n",
      "train loss:  0.19102738797664642\r\n",
      "train loss:  0.377040833234787\r\n",
      "train loss:  0.04931198060512543\r\n",
      "train loss:  0.2151636779308319\r\n",
      "train loss:  0.020586423575878143\r\n",
      "train loss:  0.12262466549873352\r\n",
      "train loss:  0.4427919089794159\r\n",
      "train loss:  0.16056180000305176\r\n",
      "train loss:  0.13188818097114563\r\n",
      "train loss:  0.16676601767539978\r\n",
      "train loss:  0.3795691728591919\r\n",
      "train loss:  0.26580044627189636\r\n",
      "train loss:  0.11656332015991211\r\n",
      "train loss:  0.1716272234916687\r\n",
      "train loss:  0.31772252917289734\r\n",
      "train loss:  0.05913659185171127\r\n",
      "train loss:  0.01882053352892399\r\n",
      "train loss:  0.14259982109069824\r\n",
      "train loss:  0.1371956169605255\r\n",
      "train loss:  0.15469089150428772\r\n",
      "train loss:  0.12163867056369781\r\n",
      "train loss:  0.17153720557689667\r\n",
      "train loss:  0.0503234826028347\r\n",
      "train loss:  0.1139010488986969\r\n",
      "train loss:  0.26032349467277527\r\n",
      "train loss:  0.07210882008075714\r\n",
      "train loss:  0.28969886898994446\r\n",
      "train loss:  0.19324085116386414\r\n",
      "train loss:  0.20962148904800415\r\n",
      "train loss:  0.19892635941505432\r\n",
      "train loss:  0.17305880784988403\r\n",
      "train loss:  0.18467943370342255\r\n",
      "train loss:  0.10556178539991379\r\n",
      "train loss:  0.24887141585350037\r\n",
      "train loss:  0.08128703385591507\r\n",
      "train loss:  0.19132348895072937\r\n",
      "train loss:  0.18524569272994995\r\n",
      "train loss:  0.02825421281158924\r\n",
      "train loss:  0.1718192994594574\r\n",
      "train loss:  0.07592930644750595\r\n",
      "train loss:  0.15954481065273285\r\n",
      "train loss:  0.6104741096496582\r\n",
      "train loss:  0.07146530598402023\r\n",
      "train loss:  0.0356869138777256\r\n",
      "train loss:  0.08621937036514282\r\n",
      "train loss:  0.03480392321944237\r\n",
      "train loss:  0.0649065375328064\r\n",
      "train loss:  0.13965831696987152\r\n",
      "train loss:  0.21525061130523682\r\n",
      "train loss:  0.06966652721166611\r\n",
      "train loss:  0.42721572518348694\r\n",
      "train loss:  0.10033190995454788\r\n",
      "train loss:  0.1849626749753952\r\n",
      "train loss:  0.027990741655230522\r\n",
      "train loss:  0.17001910507678986\r\n",
      "train loss:  0.12471269816160202\r\n",
      "train loss:  0.11144598573446274\r\n",
      "train loss:  0.09906204789876938\r\n",
      "train loss:  0.10235127806663513\r\n",
      "train loss:  0.09827934205532074\r\n",
      "train loss:  0.15032261610031128\r\n",
      "train loss:  0.5815929770469666\r\n",
      "train loss:  0.1769046187400818\r\n",
      "train loss:  0.13312053680419922\r\n",
      "train loss:  0.2795068919658661\r\n",
      "train loss:  0.3181930482387543\r\n",
      "train loss:  0.18479520082473755\r\n",
      "train loss:  0.254219651222229\r\n",
      "train loss:  0.07559158653020859\r\n",
      "train loss:  0.4479755163192749\r\n",
      "train loss:  0.3857634365558624\r\n",
      "train loss:  0.14148999750614166\r\n",
      "train loss:  0.3150339722633362\r\n",
      "train loss:  0.24997872114181519\r\n",
      "train loss:  0.3109371066093445\r\n",
      "train loss:  0.15498660504817963\r\n",
      "train loss:  0.2633737623691559\r\n",
      "train loss:  0.3264065682888031\r\n",
      "train loss:  0.11504800617694855\r\n",
      "train loss:  0.10919181257486343\r\n",
      "train loss:  0.09764992445707321\r\n",
      "train loss:  0.3695160150527954\r\n",
      "train loss:  0.21372579038143158\r\n",
      "train loss:  0.15386514365673065\r\n",
      "train loss:  0.3542096018791199\r\n",
      "train loss:  0.13795796036720276\r\n",
      "train loss:  0.27025794982910156\r\n",
      "train loss:  0.32542431354522705\r\n",
      "train loss:  0.06879260390996933\r\n",
      "train loss:  0.13603834807872772\r\n",
      "train loss:  0.08804278075695038\r\n",
      "train loss:  0.11153920739889145\r\n",
      "train loss:  0.1778494417667389\r\n",
      "train loss:  0.16488875448703766\r\n",
      "train loss:  0.5402470231056213\r\n",
      "train loss:  0.13871769607067108\r\n",
      "train loss:  0.05634362995624542\r\n",
      "train loss:  0.16547749936580658\r\n",
      "train loss:  0.06439727544784546\r\n",
      "train loss:  0.5002892017364502\r\n",
      "train loss:  0.10431241244077682\r\n",
      "train loss:  0.15845002233982086\r\n",
      "train loss:  0.0832769125699997\r\n",
      "train loss:  0.16367581486701965\r\n",
      "train loss:  0.09730183333158493\r\n",
      "train loss:  0.0519915446639061\r\n",
      "train loss:  0.27882498502731323\r\n",
      "train loss:  0.08784721046686172\r\n",
      "train loss:  0.17060069739818573\r\n",
      "train loss:  0.12045975774526596\r\n",
      "train loss:  0.20630532503128052\r\n",
      "train loss:  0.21961379051208496\r\n",
      "train loss:  0.2625584006309509\r\n",
      "train loss:  0.18690747022628784\r\n",
      "train loss:  0.3484859764575958\r\n",
      "train loss:  0.17087115347385406\r\n",
      "train loss:  0.17196860909461975\r\n",
      "train loss:  0.2732710540294647\r\n",
      "train loss:  0.11032889783382416\r\n",
      "train loss:  0.20006732642650604\r\n",
      "train loss:  0.0564446859061718\r\n",
      "train loss:  0.07894393056631088\r\n",
      "train loss:  0.07506544142961502\r\n",
      "train loss:  0.2593783736228943\r\n",
      "train loss:  0.37059110403060913\r\n",
      "train loss:  0.142682746052742\r\n",
      "train loss:  0.10965454578399658\r\n",
      "train loss:  0.1164555624127388\r\n",
      "train loss:  0.14268507063388824\r\n",
      "train loss:  0.19612650573253632\r\n",
      "train loss:  0.05098925158381462\r\n",
      "train loss:  0.09818495810031891\r\n",
      "train loss:  0.020396174862980843\r\n",
      "train loss:  0.21520979702472687\r\n",
      "train loss:  0.3952285051345825\r\n",
      "train loss:  0.07304494082927704\r\n",
      "train loss:  0.18610820174217224\r\n",
      "train loss:  0.1794523000717163\r\n",
      "train loss:  0.2992424964904785\r\n",
      "train loss:  0.13163414597511292\r\n",
      "train loss:  0.13503862917423248\r\n",
      "train loss:  0.04454938694834709\r\n",
      "train loss:  0.04952322691679001\r\n",
      "train loss:  0.30560144782066345\r\n",
      "train loss:  0.46014899015426636\r\n",
      "train loss:  0.2068580687046051\r\n",
      "train loss:  0.22257497906684875\r\n",
      "train loss:  0.3100874423980713\r\n",
      "train loss:  0.16860799491405487\r\n",
      "train loss:  0.12750732898712158\r\n",
      "train loss:  0.19018390774726868\r\n",
      "train loss:  0.38200175762176514\r\n",
      "train loss:  0.1196058988571167\r\n",
      "train loss:  0.6731097102165222\r\n",
      "train loss:  0.6816652417182922\r\n",
      "train loss:  0.28224191069602966\r\n",
      "train loss:  0.6090848445892334\r\n",
      "train loss:  0.14703796803951263\r\n",
      "train loss:  0.16114383935928345\r\n",
      "train loss:  0.556957483291626\r\n",
      "train loss:  0.18634529411792755\r\n",
      "train loss:  0.074861079454422\r\n",
      "train loss:  0.20483076572418213\r\n",
      "train loss:  0.11173751205205917\r\n",
      "train loss:  0.19633349776268005\r\n",
      "train loss:  0.22785082459449768\r\n",
      "train loss:  0.07617096602916718\r\n",
      "train loss:  0.3164982199668884\r\n",
      "train loss:  0.2716325521469116\r\n",
      "train loss:  0.1081884503364563\r\n",
      "train loss:  0.1481720209121704\r\n",
      "train loss:  0.29697751998901367\r\n",
      "train loss:  0.14863723516464233\r\n",
      "train loss:  0.20827296376228333\r\n",
      "train loss:  0.10372468829154968\r\n",
      "train loss:  0.14265161752700806\r\n",
      "train loss:  0.05932803079485893\r\n",
      "train loss:  0.1549217700958252\r\n",
      "train loss:  0.06341838091611862\r\n",
      "train loss:  0.10183468461036682\r\n",
      "train loss:  0.10357677936553955\r\n",
      "train loss:  0.2771509289741516\r\n",
      "train loss:  0.1453688144683838\r\n",
      "train loss:  0.2578206956386566\r\n",
      "train loss:  0.06656226515769958\r\n",
      "train loss:  0.35677170753479004\r\n",
      "train loss:  0.14356355369091034\r\n",
      "train loss:  0.0722159892320633\r\n",
      "train loss:  0.08607218414545059\r\n",
      "train loss:  0.379270076751709\r\n",
      "train loss:  0.04903384670615196\r\n",
      "train loss:  0.275407075881958\r\n",
      "train loss:  0.4203436076641083\r\n",
      "train loss:  0.11835414916276932\r\n",
      "train loss:  0.19866342842578888\r\n",
      "train loss:  0.2505219876766205\r\n",
      "train loss:  0.09167120605707169\r\n",
      "train loss:  0.2274121642112732\r\n",
      "train loss:  0.07573799788951874\r\n",
      "train loss:  0.06587287783622742\r\n",
      "train loss:  0.022644944489002228\r\n",
      "train loss:  0.08612988889217377\r\n",
      "train loss:  0.27076151967048645\r\n",
      "train loss:  0.11341654509305954\r\n",
      "train loss:  0.1435028314590454\r\n",
      "train loss:  0.25593793392181396\r\n",
      "train loss:  0.0716899186372757\r\n",
      "train loss:  0.04970375448465347\r\n",
      "train loss:  0.28391391038894653\r\n",
      "train loss:  0.07594536989927292\r\n",
      "train loss:  0.19539421796798706\r\n",
      "train loss:  0.10769332200288773\r\n",
      "train loss:  0.14167353510856628\r\n",
      "train loss:  0.224172443151474\r\n",
      "train loss:  0.5321829319000244\r\n",
      "train loss:  0.25957733392715454\r\n",
      "train loss:  0.31066253781318665\r\n",
      "train loss:  0.311643123626709\r\n",
      "train loss:  0.1896386444568634\r\n",
      "train loss:  0.33489733934402466\r\n",
      "train loss:  0.0709717869758606\r\n",
      "train loss:  0.21093198657035828\r\n",
      "train loss:  0.1316729485988617\r\n",
      "train loss:  0.19069522619247437\r\n",
      "train loss:  0.3863750398159027\r\n",
      "train loss:  0.2907665967941284\r\n",
      "train loss:  0.145208939909935\r\n",
      "train loss:  0.17695395648479462\r\n",
      "train loss:  0.20969189703464508\r\n",
      "train loss:  0.306660532951355\r\n",
      "train loss:  0.16043666005134583\r\n",
      "train loss:  0.23085308074951172\r\n",
      "train loss:  0.16938278079032898\r\n",
      "train loss:  0.14106327295303345\r\n",
      "train loss:  0.5478293299674988\r\n",
      "train loss:  0.2114650011062622\r\n",
      "train loss:  0.16054178774356842\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  0.090863898396492\r\n",
      "train loss:  0.2544744610786438\r\n",
      "train loss:  0.1401602327823639\r\n",
      "train loss:  0.1926317662000656\r\n",
      "train loss:  0.12522147595882416\r\n",
      "train loss:  0.09223892539739609\r\n",
      "train loss:  0.10074837505817413\r\n",
      "train loss:  0.09574346989393234\r\n",
      "train loss:  0.25459417700767517\r\n",
      "train loss:  0.07994487136602402\r\n",
      "train loss:  0.25565943121910095\r\n",
      "train loss:  0.08707111328840256\r\n",
      "train loss:  0.031752705574035645\r\n",
      "train loss:  0.258897602558136\r\n",
      "train loss:  0.16363131999969482\r\n",
      "train loss:  0.21747152507305145\r\n",
      "train loss:  0.0945231094956398\r\n",
      "train loss:  0.15799230337142944\r\n",
      "train loss:  0.11502828449010849\r\n",
      "train loss:  0.23769579827785492\r\n",
      "train loss:  0.02811853401362896\r\n",
      "train loss:  0.2178477793931961\r\n",
      "train loss:  0.24338386952877045\r\n",
      "train loss:  0.04241466522216797\r\n",
      "train loss:  0.16036589443683624\r\n",
      "train loss:  0.04613339528441429\r\n",
      "train loss:  0.1670295000076294\r\n",
      "train loss:  0.022944072261452675\r\n",
      "train loss:  0.17108307778835297\r\n",
      "train loss:  0.43607795238494873\r\n",
      "train loss:  0.19769662618637085\r\n",
      "train loss:  0.11675059795379639\r\n",
      "train loss:  0.14142552018165588\r\n",
      "train loss:  0.28812330961227417\r\n",
      "train loss:  0.034329917281866074\r\n",
      "train loss:  0.059911251068115234\r\n",
      "train loss:  0.2422621250152588\r\n",
      "train loss:  0.10056549310684204\r\n",
      "train loss:  0.2591729164123535\r\n",
      "train loss:  0.37551164627075195\r\n",
      "train loss:  0.25632426142692566\r\n",
      "train loss:  0.2407664954662323\r\n",
      "train loss:  0.15922541916370392\r\n",
      "train loss:  0.29278358817100525\r\n",
      "train loss:  0.34003952145576477\r\n",
      "train loss:  0.2633422613143921\r\n",
      "train loss:  0.17173142731189728\r\n",
      "train loss:  0.24319203197956085\r\n",
      "train loss:  0.24139107763767242\r\n",
      "train loss:  0.27392372488975525\r\n",
      "train loss:  0.06339497119188309\r\n",
      "train loss:  0.09916510432958603\r\n",
      "train loss:  0.22320367395877838\r\n",
      "train loss:  0.2193438559770584\r\n",
      "train loss:  0.20013073086738586\r\n",
      "train loss:  0.13529439270496368\r\n",
      "train loss:  0.1159825548529625\r\n",
      "train loss:  0.07869881391525269\r\n",
      "train loss:  0.1591491401195526\r\n",
      "train loss:  0.20655901730060577\r\n",
      "train loss:  0.1555200219154358\r\n",
      "train loss:  0.2588216960430145\r\n",
      "train loss:  0.2888730764389038\r\n",
      "train loss:  0.21005314588546753\r\n",
      "train loss:  0.09018164128065109\r\n",
      "train loss:  0.23180606961250305\r\n",
      "train loss:  0.08181924372911453\r\n",
      "train loss:  0.1373777985572815\r\n",
      "train loss:  0.15466320514678955\r\n",
      "train loss:  0.057983461767435074\r\n",
      "train loss:  0.030901126563549042\r\n",
      "train loss:  0.1966293603181839\r\n",
      "train loss:  0.4054674506187439\r\n",
      "train loss:  0.13512472808361053\r\n",
      "train loss:  0.14321565628051758\r\n",
      "train loss:  0.5834752917289734\r\n",
      "train loss:  0.35720518231391907\r\n",
      "train loss:  0.5907578468322754\r\n",
      "train loss:  0.12908458709716797\r\n",
      "train loss:  0.254901647567749\r\n",
      "train loss:  0.08509819954633713\r\n",
      "train loss:  0.09432362765073776\r\n",
      "train loss:  0.16434480249881744\r\n",
      "train loss:  0.24958333373069763\r\n",
      "train loss:  0.07109618186950684\r\n",
      "train loss:  0.28688251972198486\r\n",
      "train loss:  0.17347408831119537\r\n",
      "train loss:  0.35375022888183594\r\n",
      "train loss:  0.3558848798274994\r\n",
      "train loss:  0.18890397250652313\r\n",
      "train loss:  0.37178853154182434\r\n",
      "train loss:  0.38685259222984314\r\n",
      "train loss:  0.20797793567180634\r\n",
      "train loss:  0.21435973048210144\r\n",
      "train loss:  0.22142747044563293\r\n",
      "train loss:  0.21505093574523926\r\n",
      "train loss:  0.18474099040031433\r\n",
      "train loss:  0.1361379623413086\r\n",
      "train loss:  0.1726161539554596\r\n",
      "train loss:  0.20836876332759857\r\n",
      "train loss:  0.1067487820982933\r\n",
      "train loss:  0.04880353435873985\r\n",
      "train loss:  0.03579551354050636\r\n",
      "train loss:  0.2450825423002243\r\n",
      "train loss:  0.0951390191912651\r\n",
      "train loss:  0.03438100591301918\r\n",
      "train loss:  0.20810215175151825\r\n",
      "train loss:  0.11620845645666122\r\n",
      "train loss:  0.11070461571216583\r\n",
      "train loss:  0.24454450607299805\r\n",
      "train loss:  0.28057557344436646\r\n",
      "train loss:  0.07694723457098007\r\n",
      "train loss:  0.05521317943930626\r\n",
      "train loss:  0.46513262391090393\r\n",
      "train loss:  0.1287180334329605\r\n",
      "train loss:  0.016522517427802086\r\n",
      "train loss:  0.03882015869021416\r\n",
      "train loss:  0.1046610176563263\r\n",
      "train loss:  0.09985966235399246\r\n",
      "train loss:  0.07066861540079117\r\n",
      "train loss:  0.28174325823783875\r\n",
      "train loss:  0.08648847043514252\r\n",
      "train loss:  0.30716371536254883\r\n",
      "train loss:  0.3284745216369629\r\n",
      "train loss:  0.039850398898124695\r\n",
      "train loss:  0.0658135861158371\r\n",
      "train loss:  0.19909518957138062\r\n",
      "train loss:  0.26772966980934143\r\n",
      "train loss:  0.19346076250076294\r\n",
      "train loss:  0.12023501098155975\r\n",
      "train loss:  0.13795515894889832\r\n",
      "train loss:  0.1984153687953949\r\n",
      "train loss:  0.17037814855575562\r\n",
      "train loss:  0.18656636774539948\r\n",
      "train loss:  0.17312033474445343\r\n",
      "train loss:  0.23347437381744385\r\n",
      "train loss:  0.049532029777765274\r\n",
      "train loss:  0.2805689573287964\r\n",
      "train loss:  0.29134488105773926\r\n",
      "train loss:  0.26334595680236816\r\n",
      "train loss:  0.13768525421619415\r\n",
      "train loss:  0.16104692220687866\r\n",
      "train loss:  0.08786140382289886\r\n",
      "train loss:  0.1401549130678177\r\n",
      "train loss:  0.09507995843887329\r\n",
      "train loss:  0.0872485339641571\r\n",
      "train loss:  0.09923271089792252\r\n",
      "train loss:  0.24952149391174316\r\n",
      "train loss:  0.04309088736772537\r\n",
      "train loss:  0.25997281074523926\r\n",
      "train loss:  0.2553722560405731\r\n",
      "train loss:  0.2884950339794159\r\n",
      "train loss:  0.24213311076164246\r\n",
      "train loss:  0.15004095435142517\r\n",
      "train loss:  0.3476168215274811\r\n",
      "train loss:  0.16112081706523895\r\n",
      "train loss:  0.1543091982603073\r\n",
      "train loss:  0.21186865866184235\r\n",
      "train loss:  0.31145378947257996\r\n",
      "train loss:  0.21936337649822235\r\n",
      "train loss:  0.09340639412403107\r\n",
      "train loss:  0.2541411519050598\r\n",
      "train loss:  0.12085509300231934\r\n",
      "train loss:  0.2300732433795929\r\n",
      "train loss:  0.186238631606102\r\n",
      "train loss:  0.037642527371644974\r\n",
      "train loss:  0.22323738038539886\r\n",
      "train loss:  0.08020330220460892\r\n",
      "train loss:  0.2592884302139282\r\n",
      "train loss:  0.2096078246831894\r\n",
      "train loss:  0.21249160170555115\r\n",
      "train loss:  0.40579479932785034\r\n",
      "train loss:  0.10721912980079651\r\n",
      "train loss:  0.3732650578022003\r\n",
      "train loss:  0.4327067732810974\r\n",
      "train loss:  0.10313020646572113\r\n",
      "train loss:  0.1456436812877655\r\n",
      "train loss:  0.03277519717812538\r\n",
      "train loss:  0.19572748243808746\r\n",
      "train loss:  0.37409257888793945\r\n",
      "train loss:  0.5298939347267151\r\n",
      "train loss:  0.07217954099178314\r\n",
      "train loss:  0.249144047498703\r\n",
      "train loss:  0.32922104001045227\r\n",
      "train loss:  0.11927163600921631\r\n",
      "train loss:  0.2564980089664459\r\n",
      "train loss:  0.081487737596035\r\n",
      "train loss:  0.23030070960521698\r\n",
      "train loss:  0.27795398235321045\r\n",
      "train loss:  0.2877518832683563\r\n",
      "train loss:  0.3282984793186188\r\n",
      "train loss:  0.21371646225452423\r\n",
      "train loss:  0.09188792109489441\r\n",
      "train loss:  0.03578461334109306\r\n",
      "train loss:  0.534143328666687\r\n",
      "train loss:  0.4323473274707794\r\n",
      "train loss:  0.10721931606531143\r\n",
      "train loss:  0.280514121055603\r\n",
      "train loss:  0.23374304175376892\r\n",
      "train loss:  0.21100500226020813\r\n",
      "train loss:  0.2539483606815338\r\n",
      "train loss:  0.058698318898677826\r\n",
      "train loss:  0.037195924669504166\r\n",
      "train loss:  0.054414067417383194\r\n",
      "train loss:  0.12607444822788239\r\n",
      "train loss:  0.15420641005039215\r\n",
      "train loss:  0.07993882149457932\r\n",
      "train loss:  0.015818670392036438\r\n",
      "train loss:  0.1339924931526184\r\n",
      "train loss:  0.4937945008277893\r\n",
      "train loss:  0.14100666344165802\r\n",
      "train loss:  0.11106660217046738\r\n",
      "train loss:  0.05476441606879234\r\n",
      "train loss:  0.1798662692308426\r\n",
      "train loss:  0.11352749168872833\r\n",
      "train loss:  0.10771963000297546\r\n",
      "train loss:  0.07538212835788727\r\n",
      "train loss:  0.19064484536647797\r\n",
      "train loss:  0.2132526934146881\r\n",
      "train loss:  0.03674471750855446\r\n",
      "train loss:  0.06276512145996094\r\n",
      "train loss:  0.10842511802911758\r\n",
      "train loss:  0.15719467401504517\r\n",
      "train loss:  0.04413269832730293\r\n",
      "train loss:  0.24617999792099\r\n",
      "train loss:  0.455739825963974\r\n",
      "train loss:  0.369658499956131\r\n",
      "train loss:  0.17151330411434174\r\n",
      "train loss:  0.32545092701911926\r\n",
      "train loss:  0.17574547231197357\r\n",
      "train loss:  0.19897884130477905\r\n",
      "train loss:  0.11446874588727951\r\n",
      "train loss:  0.3811555802822113\r\n",
      "train loss:  0.04679148271679878\r\n",
      "train loss:  0.044020675122737885\r\n",
      "train loss:  0.13649982213974\r\n",
      "train loss:  0.5848470330238342\r\n",
      "train loss:  0.17633375525474548\r\n",
      "train loss:  0.10819745063781738\r\n",
      "train loss:  0.3232489824295044\r\n",
      "train loss:  0.2072640359401703\r\n",
      "train loss:  0.25476187467575073\r\n",
      "train loss:  0.07820212841033936\r\n",
      "train loss:  0.019690025597810745\r\n",
      "train loss:  0.11771456897258759\r\n",
      "train loss:  0.12415166199207306\r\n",
      "train loss:  0.43541014194488525\r\n",
      "train loss:  0.30198538303375244\r\n",
      "train loss:  0.12702898681163788\r\n",
      "train loss:  0.1412445604801178\r\n",
      "train loss:  0.10253560543060303\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  0.1032118946313858\r\n",
      "train loss:  0.3838052749633789\r\n",
      "train loss:  0.1282661259174347\r\n",
      "train loss:  0.0392349511384964\r\n",
      "train loss:  0.029450083151459694\r\n",
      "train loss:  0.10374301671981812\r\n",
      "train loss:  0.2574203610420227\r\n",
      "train loss:  0.09947174042463303\r\n",
      "train loss:  0.13192367553710938\r\n",
      "train loss:  0.4322797656059265\r\n",
      "train loss:  0.26139363646507263\r\n",
      "train loss:  0.0077400109730660915\r\n",
      "train loss:  0.09614505618810654\r\n",
      "train loss:  0.10511835664510727\r\n",
      "train loss:  0.21159441769123077\r\n",
      "train loss:  0.056338194757699966\r\n",
      "train loss:  0.12064818292856216\r\n",
      "train loss:  0.0382717065513134\r\n",
      "train loss:  0.3465595841407776\r\n",
      "train loss:  0.08657026290893555\r\n",
      "train loss:  0.08317553251981735\r\n",
      "train loss:  0.05137413367629051\r\n",
      "train loss:  0.03955783322453499\r\n",
      "train loss:  0.044798873364925385\r\n",
      "train loss:  0.05747095122933388\r\n",
      "train loss:  0.06586723774671555\r\n",
      "train loss:  0.1815185248851776\r\n",
      "train loss:  0.5255250334739685\r\n",
      "train loss:  0.5075241923332214\r\n",
      "train loss:  0.5348948836326599\r\n",
      "train loss:  0.2777853012084961\r\n",
      "train loss:  0.23090197145938873\r\n",
      "train loss:  0.1438785046339035\r\n",
      "train loss:  0.10860522836446762\r\n",
      "train loss:  0.07243704795837402\r\n",
      "train loss:  0.348203182220459\r\n",
      "train loss:  0.29480692744255066\r\n",
      "train loss:  0.18798957765102386\r\n",
      "train loss:  0.1585962474346161\r\n",
      "train loss:  0.0756898745894432\r\n",
      "train loss:  0.027433624491095543\r\n",
      "train loss:  0.05529402568936348\r\n",
      "train loss:  0.19505424797534943\r\n",
      "train loss:  0.17030401527881622\r\n",
      "train loss:  0.5489945411682129\r\n",
      "train loss:  0.3592820167541504\r\n",
      "train loss:  0.4370335340499878\r\n",
      "train loss:  0.4403455853462219\r\n",
      "train loss:  0.2688676416873932\r\n",
      "train loss:  0.21174006164073944\r\n",
      "train loss:  0.2503718435764313\r\n",
      "train loss:  0.09644008427858353\r\n",
      "train loss:  0.14638522267341614\r\n",
      "train loss:  0.041358646005392075\r\n",
      "train loss:  0.09790182113647461\r\n",
      "train loss:  0.20944297313690186\r\n",
      "train loss:  0.18038812279701233\r\n",
      "train loss:  0.8021071553230286\r\n",
      "train loss:  0.15444530546665192\r\n",
      "train loss:  0.12044653296470642\r\n",
      "train loss:  0.18248850107192993\r\n",
      "train loss:  0.05465816706418991\r\n",
      "train loss:  0.15665458142757416\r\n",
      "train loss:  0.2076968252658844\r\n",
      "train loss:  0.10396397858858109\r\n",
      "train loss:  0.07800262421369553\r\n",
      "train loss:  0.10866129398345947\r\n",
      "train loss:  0.5522127747535706\r\n",
      "train loss:  0.1304132640361786\r\n",
      "train loss:  0.15292119979858398\r\n",
      "train loss:  0.3712044954299927\r\n",
      "train loss:  0.24977877736091614\r\n",
      "train loss:  0.3402957320213318\r\n",
      "train loss:  0.768927276134491\r\n",
      "train loss:  0.12750844657421112\r\n",
      "train loss:  0.06142831966280937\r\n",
      "train loss:  0.423768013715744\r\n",
      "train loss:  0.06275144219398499\r\n",
      "train loss:  0.39818260073661804\r\n",
      "train loss:  0.099022775888443\r\n",
      "train loss:  0.35923758149147034\r\n",
      "train loss:  0.04764872044324875\r\n",
      "train loss:  0.3402343690395355\r\n",
      "train loss:  0.148458331823349\r\n",
      "train loss:  0.12754762172698975\r\n",
      "train loss:  0.07354199141263962\r\n",
      "train loss:  0.1569957137107849\r\n",
      "train loss:  0.044978462159633636\r\n",
      "train loss:  0.1387435793876648\r\n",
      "train loss:  0.06440957635641098\r\n",
      "train loss:  0.04442145675420761\r\n",
      "train loss:  0.10473985970020294\r\n",
      "train loss:  0.04820825904607773\r\n",
      "train loss:  0.10858066380023956\r\n",
      "train loss:  0.05211859568953514\r\n",
      "train loss:  0.10140417516231537\r\n",
      "train loss:  0.2707294523715973\r\n",
      "train loss:  0.10185135155916214\r\n",
      "train loss:  0.23867377638816833\r\n",
      "train loss:  0.20304234325885773\r\n",
      "train loss:  0.26602402329444885\r\n",
      "train loss:  0.159480020403862\r\n",
      "train loss:  0.19470426440238953\r\n",
      "train loss:  0.08943767100572586\r\n",
      "train loss:  0.07690707594156265\r\n",
      "train loss:  0.09385094791650772\r\n",
      "train loss:  0.28302469849586487\r\n",
      "train loss:  0.016679517924785614\r\n",
      "train loss:  0.3418944180011749\r\n",
      "train loss:  0.013454478234052658\r\n",
      "train loss:  0.14177922904491425\r\n",
      "train loss:  0.21185213327407837\r\n",
      "train loss:  0.08370363712310791\r\n",
      "train loss:  0.24102157354354858\r\n",
      "train loss:  0.27148178219795227\r\n",
      "train loss:  0.17198510468006134\r\n",
      "train loss:  0.09913383424282074\r\n",
      "train loss:  0.022856950759887695\r\n",
      "train loss:  0.10614839196205139\r\n",
      "train loss:  0.07284072041511536\r\n",
      "train loss:  0.023053474724292755\r\n",
      "train loss:  0.18743325769901276\r\n",
      "train loss:  0.2516385614871979\r\n",
      "train loss:  0.09212144464254379\r\n",
      "train loss:  0.14784538745880127\r\n",
      "train loss:  0.1338534951210022\r\n",
      "train loss:  0.13908153772354126\r\n",
      "train loss:  0.3992641568183899\r\n",
      "train loss:  0.2637089788913727\r\n",
      "train loss:  0.13796980679035187\r\n",
      "train loss:  0.2556864619255066\r\n",
      "train loss:  0.08028943836688995\r\n",
      "train loss:  0.09515565633773804\r\n",
      "train loss:  0.17001698911190033\r\n",
      "train loss:  0.11351554095745087\r\n",
      "train loss:  0.18502023816108704\r\n",
      "train loss:  0.18267612159252167\r\n",
      "train loss:  0.2287030965089798\r\n",
      "train loss:  0.030531272292137146\r\n",
      "train loss:  0.03489498421549797\r\n",
      "train loss:  0.1410561203956604\r\n",
      "train loss:  0.0348709300160408\r\n",
      "train loss:  0.08176057785749435\r\n",
      "train loss:  0.11675546318292618\r\n",
      "train loss:  0.2998802065849304\r\n",
      "train loss:  0.2981676161289215\r\n",
      "train loss:  0.21459685266017914\r\n",
      "train loss:  0.06860853731632233\r\n",
      "train loss:  0.37505272030830383\r\n",
      "train loss:  0.12047576159238815\r\n",
      "train loss:  0.13461510837078094\r\n",
      "train loss:  0.4359246790409088\r\n",
      "train loss:  0.5719365477561951\r\n",
      "train loss:  0.5562825202941895\r\n",
      "train loss:  0.1545083075761795\r\n",
      "train loss:  0.18389111757278442\r\n",
      "train loss:  0.061376363039016724\r\n",
      "train loss:  0.15443870425224304\r\n",
      "train loss:  0.19365181028842926\r\n",
      "train loss:  0.42641904950141907\r\n",
      "train loss:  0.09111060202121735\r\n",
      "train loss:  0.2541687786579132\r\n",
      "train loss:  0.024782169610261917\r\n",
      "train loss:  0.07546001672744751\r\n",
      "train loss:  0.05766729637980461\r\n",
      "train loss:  0.014051984064280987\r\n",
      "train loss:  0.09559618681669235\r\n",
      "train loss:  0.01619439572095871\r\n",
      "train loss:  0.13118867576122284\r\n",
      "train loss:  0.2216673493385315\r\n",
      "train loss:  0.14422419667243958\r\n",
      "train loss:  0.3185575604438782\r\n",
      "train loss:  0.3654579818248749\r\n",
      "train loss:  0.2244376540184021\r\n",
      "train loss:  0.31839874386787415\r\n",
      "train loss:  0.21986927092075348\r\n",
      "train loss:  0.027254775166511536\r\n",
      "train loss:  0.1790708303451538\r\n",
      "train loss:  0.025734683498740196\r\n",
      "train loss:  0.1679171323776245\r\n",
      "train loss:  0.28309863805770874\r\n",
      "train loss:  0.18021279573440552\r\n",
      "train loss:  0.36857956647872925\r\n",
      "train loss:  0.029075026512145996\r\n",
      "train loss:  0.2965134382247925\r\n",
      "train loss:  0.30928099155426025\r\n",
      "train loss:  0.07069569826126099\r\n",
      "train loss:  0.15634296834468842\r\n",
      "train loss:  0.2470925897359848\r\n",
      "train loss:  0.027953892946243286\r\n",
      "train loss:  0.028602739796042442\r\n",
      "train loss:  0.33948948979377747\r\n",
      "train loss:  0.024816595017910004\r\n",
      "train loss:  0.10915204882621765\r\n",
      "train loss:  0.1432996392250061\r\n",
      "train loss:  0.12225046753883362\r\n",
      "train loss:  0.034547556191682816\r\n",
      "train loss:  0.2539984881877899\r\n",
      "train loss:  0.07633264362812042\r\n",
      "train loss:  0.18722279369831085\r\n",
      "train loss:  0.04837558791041374\r\n",
      "train loss:  0.2125011533498764\r\n",
      "train loss:  0.1814260482788086\r\n",
      "train loss:  0.21217074990272522\r\n",
      "train loss:  0.04043250530958176\r\n",
      "train loss:  0.11523201316595078\r\n",
      "train loss:  0.043703071773052216\r\n",
      "train loss:  0.06603382527828217\r\n",
      "train loss:  0.021497825160622597\r\n",
      "train loss:  0.24563166499137878\r\n",
      "train loss:  0.08587756007909775\r\n",
      "train loss:  0.05210686847567558\r\n",
      "train loss:  0.3135271370410919\r\n",
      "train loss:  0.34164687991142273\r\n",
      "train loss:  0.3567620813846588\r\n",
      "train loss:  0.28962889313697815\r\n",
      "train loss:  0.09477435797452927\r\n",
      "train loss:  0.11808554083108902\r\n",
      "train loss:  0.09063323587179184\r\n",
      "train loss:  0.04867114499211311\r\n",
      "train loss:  0.1302284151315689\r\n",
      "train loss:  0.020122181624174118\r\n",
      "train loss:  0.28358983993530273\r\n",
      "train loss:  0.07543311268091202\r\n",
      "train loss:  0.1174464076757431\r\n",
      "train loss:  0.20547352731227875\r\n",
      "train loss:  0.12781597673892975\r\n",
      "train loss:  0.3465883135795593\r\n",
      "train loss:  0.03365940973162651\r\n",
      "train loss:  0.04849908500909805\r\n",
      "train loss:  0.1752796322107315\r\n",
      "train loss:  0.13658076524734497\r\n",
      "train loss:  0.3106997311115265\r\n",
      "train loss:  0.06719007343053818\r\n",
      "train loss:  0.02372060716152191\r\n",
      "train loss:  0.131456196308136\r\n",
      "train loss:  0.016864199191331863\r\n",
      "train loss:  0.06676103174686432\r\n",
      "train loss:  0.019777527078986168\r\n",
      "train loss:  0.32809972763061523\r\n",
      "train loss:  0.05718965083360672\r\n",
      "train loss:  0.21574097871780396\r\n",
      "train loss:  0.13346508145332336\r\n",
      "train loss:  0.11987128853797913\r\n",
      "train loss:  0.1552446186542511\r\n",
      "train loss:  0.26231279969215393\r\n",
      "train loss:  0.12018165737390518\r\n",
      "train loss:  0.01892765611410141\r\n",
      "train loss:  0.20970867574214935\r\n",
      "train loss:  0.08380687236785889\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  0.365198016166687\n",
      "train loss:  0.025993643328547478\n",
      "train loss:  0.12650632858276367\n",
      "train loss:  0.13067449629306793\n",
      "train loss:  0.23009370267391205\n",
      "train loss:  0.17276373505592346\n",
      "train loss:  0.10914704948663712\n",
      "train loss:  0.18575461208820343\n",
      "train loss:  0.036218516528606415\n",
      "train loss:  0.11813253909349442\n",
      "train loss:  0.32182618975639343\n",
      "train loss:  0.01901237852871418\n",
      "train loss:  0.3432166278362274\n",
      "train loss:  0.1858583688735962\n",
      "train loss:  0.043809596449136734\n",
      "train loss:  0.01375996321439743\n",
      "train loss:  0.1909327208995819\n",
      "train loss:  0.06649475544691086\n",
      "train loss:  0.21271397173404694\n",
      "train loss:  0.06829574704170227\n",
      "train loss:  0.17352963984012604\n",
      "train loss:  0.13698029518127441\n",
      "train loss:  0.06564821302890778\n",
      "train loss:  0.12472058087587357\n",
      "train loss:  0.1047203466296196\n",
      "train loss:  0.07698763161897659\n",
      "train loss:  0.07048959285020828\n",
      "train loss:  0.017127644270658493\n",
      "train loss:  0.03655655309557915\n",
      "train loss:  0.18553860485553741\n",
      "train loss:  0.14210882782936096\n",
      "train loss:  0.12588171660900116\n",
      "train loss:  0.02915951795876026\n",
      "train loss:  0.019735736772418022\n",
      "train loss:  0.09723570942878723\n",
      "train loss:  0.03801254555583\n",
      "train loss:  0.07444486021995544\n",
      "train loss:  0.16261953115463257\n",
      "train loss:  0.22604703903198242\n",
      "train loss:  0.27856919169425964\n",
      "train loss:  0.036526985466480255\n",
      "train loss:  0.1264656037092209\n",
      "train loss:  0.03954384848475456\n",
      "train loss:  0.12591519951820374\n",
      "train loss:  0.12716184556484222\n",
      "train loss:  0.12040428817272186\n",
      "train loss:  0.08777769654989243\n",
      "train loss:  0.036738768219947815\n",
      "train loss:  0.18473979830741882\n",
      "train loss:  0.20120593905448914\n",
      "train loss:  0.11305852234363556\n",
      "train loss:  0.16325373947620392\n",
      "train loss:  0.2791607081890106\n",
      "train loss:  0.144937664270401\n",
      "train loss:  0.16333931684494019\n",
      "train loss:  0.08254232257604599\n",
      "train loss:  0.08888274431228638\n",
      "train loss:  0.03820767253637314\n",
      "train loss:  0.0433095321059227\n",
      "train loss:  0.21157075464725494\n",
      "train loss:  0.024454902857542038\n",
      "train loss:  0.07371475547552109\n",
      "train loss:  0.20194630324840546\n",
      "train loss:  0.09961889684200287\n",
      "train loss:  0.07496287673711777\n",
      "train loss:  0.030702896416187286\n",
      "train loss:  0.01801132969558239\n",
      "train loss:  0.08835115283727646\n",
      "train loss:  0.06901383399963379\n",
      "train loss:  0.028794554993510246\n",
      "train loss:  0.044125378131866455\n",
      "train loss:  0.06417281180620193\n",
      "train loss:  0.08294128626585007\n",
      "train loss:  0.057585470378398895\n",
      "train loss:  0.015115384943783283\n",
      "train loss:  0.06644559651613235\n",
      "train loss:  0.011195294559001923\n",
      "train loss:  0.040103182196617126\n",
      "train loss:  0.048678915947675705\n",
      "train loss:  0.07187192142009735\n",
      "train loss:  0.1806357204914093\n",
      "train loss:  0.03238925337791443\n",
      "train loss:  0.016963910311460495\n",
      "train loss:  0.028581947088241577\n",
      "train loss:  0.03487690910696983\n",
      "train loss:  0.1666971892118454\n",
      "train loss:  0.09534285962581635\n",
      "train loss:  0.1858438402414322\n",
      "train loss:  0.043149545788764954\n",
      "train loss:  0.006971714086830616\n",
      "train loss:  0.006885071285068989\n",
      "train loss:  0.02553974650800228\n",
      "train loss:  0.006862379144877195\n",
      "train loss:  0.035217974334955215\n",
      "train loss:  0.007797902449965477\n",
      "train loss:  0.0128406947478652\n",
      "train loss:  0.008264514617621899\n",
      "train loss:  0.007792665623128414\n",
      "train loss:  0.05367136746644974\n",
      "train loss:  0.034778330475091934\n",
      "train loss:  0.10149601846933365\n",
      "train loss:  0.24085316061973572\n",
      "train loss:  0.17149586975574493\n",
      "train loss:  0.3191511034965515\n",
      "train loss:  0.19363611936569214\n",
      "train loss:  0.05212023854255676\n",
      "train loss:  0.0060777198523283005\n",
      "train loss:  0.0038765587378293276\n",
      "train loss:  0.004934614058583975\n",
      "train loss:  0.059436529874801636\n",
      "train loss:  0.023136356845498085\n",
      "train loss:  0.02771647274494171\n",
      "train loss:  0.1941201239824295\n",
      "train loss:  0.22581276297569275\n",
      "train loss:  1.092318058013916\n",
      "train loss:  0.19658975303173065\n",
      "train loss:  0.060621071606874466\n",
      "train loss:  0.03938638046383858\n",
      "train loss:  0.004149943590164185\n",
      "train loss:  0.007423427887260914\n",
      "train loss:  0.6123118996620178\n",
      "train loss:  0.0930151641368866\n",
      "train loss:  0.051370926201343536\n",
      "valid loss:  0.16653446853160858\n",
      "/home/user/.local/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:171: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  \"torch.distributed.reduce_op is deprecated, please use \"\n",
      "Processing events...\n",
      "Saving temporary \"/tmp/nsys-report-db56-cac8-ad6c-520c.qdstrm\" file to disk...\n",
      "\n",
      "Creating final output files...\n",
      "Processing [===============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-db56-cac8-ad6c-520c.qdrep\"\n",
      "Exporting 3531212 events: [================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-db56-cac8-ad6c-520c.sqlite\n",
      "Report file moved to \"/home/user/1_VIDHYA/DLPROF/DEMO/./nsys_profile.qdrep\"\n",
      "Report file moved to \"/home/user/1_VIDHYA/DLPROF/DEMO/./nsys_profile.sqlite\"\n",
      "\n",
      "[DLProf-09:58:47] DLprof completed system call successfully\n",
      "sh: 1: python: not found\n",
      "[DLProf-09:58:48] Initializing Nsight Systems database\n",
      "[DLProf-09:59:09] Reading System Information from Nsight Systems database\n",
      "[DLProf-09:59:09] Reading Domains from Nsight Systems database\n",
      "[DLProf-09:59:11] Reading Ops from Nsight Systems database\n",
      "[DLProf-09:59:29] Reading CUDA API calls from Nsight Systems database\n",
      "[DLProf-09:59:33] Correlating network models with kernel and timeline data\n",
      "[DLProf-09:59:33] Found 1876 iterations using key_op \"/module/backward\"\n",
      "Iterations: [3746952918, 3803231682, 3839284637, 3875476326, 3929852405, 3965746799, 4001673658, 4037769632, 4073973976, 4110115069, 4145849650, 4177972912, 4209046427, 4240016919, 4271395124, 4302412687, 4333471726, 4364696582, 4396087403, 4427380303, 4458662479, 4489882481, 4521131185, 4552387230, 4583736795, 4614820756, 4646120908, 4677271195, 4708969115, 4740052471, 4771020556, 4802087709, 4833060349, 4864089682, 4895115585, 4926113497, 4957140867, 4988231036, 5019182265, 5050121092, 5081157373, 5112222700, 5143307776, 5174739465, 5206336554, 5238158042, 5270065389, 5301844014, 5333556727, 5364781179, 5395229033, 5438950233, 5491901065, 5527181842, 5577045293, 5612463072, 5647850699, 5683209546, 5718393752, 5769375582, 5804537472, 5840039868, 5875511160, 5910714526, 5946169861, 5981473167, 6015315117, 6045579950, 6076012867, 6106718997, 6137769209, 6168820585, 6199979220, 6230866604, 6278751006, 6310061736, 6341216391, 6372457905, 6403534353, 6434542468, 6465701280, 6496770363, 6527185361, 6578063154, 6609307543, 6641195990, 6672133643, 6703241245, 6734455652, 6765898404, 6797104766, 6827251009, 6866036155, 6897293005, 6928489725, 6959457447, 6990434351, 7021482141, 7052414764, 7083593948, 7114635719, 7145525527, 7176710454, 7207822561, 7238673152, 7269153796, 7299883991, 7331356106, 7362522633, 7394656959, 7425835143, 7456995668, 7488026746, 7518944853, 7549832763, 7581031607, 7612063755, 7643061533, 7687889776, 7718980998, 7749761672, 7780806859, 7811769359, 7842505780, 7873514965, 7904330577, 7935136945, 7966006947, 7996844567, 8027583760, 8058463027, 8089390157, 8120210425, 8150957099, 8181858020, 8212832589, 8243605239, 8274433430, 8305361674, 8336419511, 8367334373, 8398265682, 8430669816, 8462001537, 8493045309, 8523986242, 8555018984, 8585947008, 8616810181, 8647172819, 8689974706, 8720482010, 8750958494, 8793027889, 8822934342, 8871362229, 8906621052, 8942053404, 8977580643, 9013443476, 9049223692, 9084980052, 9120852661, 9156708625, 9192277314, 9227920634, 9262205546, 9293234009, 9323801930, 9354434480, 9385136519, 9415184206, 9454187060, 9484662145, 9514867176, 9567101097, 9614903766, 9655120614, 9685413632, 9715789564, 9746117458, 9776868950, 9807464770, 9838048687, 9868784898, 9899761511, 9940768299, 9971163857, 10021121954, 10052440436, 10083386577, 10113779293, 10144536666, 10175135042, 10229191453, 10260108756, 10290919652, 10321672148, 10352562442, 10383441620, 10413896988, 10444335816, 10474577553, 10505158029, 10536011933, 10566873750, 10597469633, 10627780560, 10658174679, 10700488216, 10736244326, 10771291075, 10822432117, 10858005385, 10893921469, 10929637997, 10965264218, 11000891819, 11036532150, 11072284128, 11104993139, 11135983998, 11167001687, 11208522729, 11238579785, 11269160906, 11299562173, 11330009021, 11360710544, 11391064765, 11435611482, 11466588877, 11497881426, 11528892769, 11559442699, 11589906356, 11620750377, 11651450423, 11682017603, 11712796360, 11743697484, 11774487992, 11805265823, 11835288975, 11885647124, 11919059056, 11964058459, 11995133481, 12042738226, 12073817785, 12104758740, 12135203378, 12185262588, 12215332151, 12259221367, 12302261656, 12332192074, 12362563772, 12393343752, 12431295172, 12462245131, 12493505754, 12525417603, 12556383183, 12587072205, 12641053872, 12676653132, 12712199420, 12748266805, 12784205179, 12819839662, 12855338329, 12890915524, 12927203982, 12963136586, 12999079160, 13030807957, 13062161213, 13093190354, 13124327751, 13155343060, 13194495433, 13225537872, 13256418839, 13287444688, 13319244740, 13363550088, 13394060958, 13424929540, 13455281155, 13501533593, 13532491104, 13563379832, 13594391954, 13625367605, 13655319639, 13703325600, 13734040144, 13765062638, 13796078433, 13826782955, 13857425632, 13888343061, 13919055653, 13963521213, 14012712478, 14043514917, 14074211637, 14104920791, 14135951443, 14166560925, 14197357413, 14227753012, 14258389431, 14289290832, 14319696918, 14353306260, 14384098940, 14414472825, 14444974806, 14475529067, 14506275144, 14536912224, 14567603200, 14598485083, 14629311811, 14660472318, 14691129676, 14734820699, 14765969262, 14796955803, 14827215870, 14877303197, 14912896184, 14948635766, 14983461100, 15029928270, 15064756730, 15100282861, 15135322242, 15181476359, 15217340008, 15253348686, 15289131844, 15324753042, 15360531217, 15395502176, 15426311325, 15457277903, 15488306730, 15531563117, 15562905607, 15611009899, 15661466104, 15692511816, 15723150710, 15770881633, 15802393784, 15833545930, 15865036507, 15895335180, 15946269669, 15977152732, 16007135978, 16037540624, 16068132252, 16098805010, 16129739560, 16160415532, 16191281579, 16222384816, 16253237513, 16284459533, 16315341923, 16346024634, 16377037989, 16407752481, 16438433960, 16469209779, 16500112710, 16529978234, 16560366399, 16591495789, 16622299631, 16652931639, 16683957903, 16714659974, 16745727735, 16776650938, 16807420543, 16838335462, 16869386913, 16900239208, 16931103586, 16961932335, 16992890822, 17023793298, 17054790770, 17085730111, 17116594957, 17147342823, 17189925133, 17220635893, 17251391704, 17282159402, 17312878454, 17344441130, 17375895219, 17406886914, 17437556848, 17468505702, 17499368628, 17540728685, 17571438229, 17602189901, 17632819963, 17663668712, 17694385754, 17725056857, 17755948007, 17786597687, 17817319282, 17848197178, 17879057954, 17926179387, 17957002410, 17987808416, 18018583114, 18050876085, 18081955071, 18112982516, 18143661865, 18174299725, 18205296137, 18236165654, 18267024556, 18297746988, 18328574647, 18359337772, 18390044335, 18420835613, 18451697944, 18482439283, 18513383011, 18544110529, 18574773930, 18604771556, 18635383813, 18683208283, 18724886036, 18755782369, 18786631438, 18817383996, 18847949117, 18878880281, 18926688941, 18957362243, 18987981227, 19018645230, 19049452946, 19080369380, 19111092520, 19154716981, 19189970153, 19225457582, 19261112394, 19296664081, 19332283748, 19367819815, 19415157053, 19445825826, 19476705466, 19507128078, 19553351599, 19584057721, 19614994873, 19664336452, 19695434228, 19726317807, 19757458353, 19788357592, 19819059382, 19850032163, 19881266743, 19911236892, 19955334117, 20003066768, 20051754105, 20087232884, 20122559309, 20158230386, 20193746742, 20228706514, 20264102893, 20300143842, 20336392888, 20371467292, 20427125769, 20463435137, 20512619465, 20543774631, 20574546205, 20605496910, 20636252802, 20666984768, 20697772009, 20728743927, 20760082559, 20791033498, 20821913006, 20852459247, 20883383426, 20914259430, 20945234870, 20976181333, 21007108596, 21037931696, 21068962214, 21099870997, 21130820972, 21161728564, 21193024914, 21223944748, 21254823520, 21285487775, 21315395105, 21363413403, 21398891847, 21451817876, 21487679514, 21523458609, 21559339352, 21611553553, 21642483883, 21673432901, 21703316439, 21752935554, 21783210273, 21827612648, 21857450749, 21887227375, 21948001926, 21983586300, 22019324655, 22055155212, 22094702677, 22125385215, 22156142654, 22187016866, 22217855553, 22248562616, 22279603005, 22310493231, 22341110914, 22371230332, 22415055432, 22455870878, 22485787210, 22516013373, 22546308552, 22576826488, 22608034660, 22638834228, 22669790776, 22700588432, 22731600497, 22788725514, 22824623766, 22860686638, 22896860478, 22933140584, 22970665806, 23007047292, 23046747405, 23076590984, 23107118002, 23137734800, 23168433692, 23199364806, 23230265917, 23261117381, 23292069831, 23322641679, 23353587108, 23384835211, 23415874022, 23446654558, 23477570925, 23508497295, 23539352722, 23581395037, 23612373501, 23643093055, 23696937387, 23727228916, 23772941863, 23808014734, 23843438129, 23878840048, 23914544881, 23950076077, 23985684965, 24019613319, 24050301787, 24080767606, 24111465489, 24141817804, 24172478640, 24203282410, 24233898464, 24264793966, 24295727490, 24326237697, 24356803011, 24387090953, 24436683530, 24467498101, 24498264338, 24529260682, 24559316924, 24605517279, 24641147782, 24676823822, 24712455223, 24748218706, 24783355767, 24830419683, 24860913059, 24891277712, 24938268825, 24969060663, 25000032149, 25031141533, 25088331456, 25124029820, 25159903965, 25195524106, 25231123982, 25266613628, 25299317072, 25355189268, 25392646449, 25423183981, 25467051529, 25515223516, 25562350623, 25592472762, 25623313648, 25669705099, 25700870518, 25731674946, 25762477707, 25793711030, 25824924554, 25856059685, 25887162881, 25918214599, 25949656749, 25980740526, 26012247793, 26043466363, 26074630066, 26106033958, 26137065737, 26167920891, 26199157960, 26230376166, 26261568817, 26292828861, 26324264037, 26355259969, 26386206043, 26417303570, 26448324015, 26479463784, 26510707446, 26541716982, 26573044422, 26604191359, 26635208721, 26666200743, 26697569464, 26728684048, 26759944905, 26791064274, 26836348414, 26867744881, 26899022194, 26930266582, 26961458557, 26992688322, 27023921233, 27055029344, 27086012922, 27117321436, 27148699505, 27179815326, 27210950134, 27242175927, 27273440282, 27304732204, 27336579882, 27368115083, 27399414485, 27430407879, 27461414199, 27492613702, 27523933659, 27554802880, 27585781630, 27617009274, 27648079971, 27679128532, 27710339629, 27742657233, 27774166046, 27805404761, 27836561914, 27867723739, 27910818350, 27942301941, 27973329393, 28004619217, 28035815956, 28066643906, 28097889921, 28128899386, 28159879570, 28209146917, 28240354463, 28271455552, 28302571595, 28333786653, 28364868697, 28396446411, 28427375234, 28458098776, 28489093179, 28520256780, 28551195111, 28582151274, 28613243942, 28644273873, 28675220460, 28706348488, 28737414968, 28768369887, 28799624752, 28830463713, 28861496331, 28892643100, 28924594428, 28956006451, 28987161180, 29018299825, 29049276890, 29080434029, 29111435010, 29142524982, 29173470244, 29204624275, 29235716741, 29266637624, 29297638893, 29328771716, 29360129632, 29391038148, 29440797991, 29471566148, 29502693425, 29533492126, 29564571332, 29595199456, 29646949498, 29689938219, 29720367355, 29750998635, 29782060992, 29813213862, 29844245261, 29875051897, 29926396646, 29961553106, 29996519927, 30031826803, 30066891840, 30097538644, 30128429767, 30159222165, 30189998071, 30220824663, 30251600917, 30282301238, 30313172183, 30343213866, 30397123407, 30437024018, 30467269110, 30497583503, 30528095379, 30558255521, 30588737265, 30619846454, 30650772501, 30681759109, 30713094041, 30744084878, 30775200076, 30806357577, 30837340189, 30868487599, 30899696653, 30930751346, 30961894095, 30993114798, 31023928639, 31054453452, 31085335656, 31116504398, 31147429291, 31178436939, 31209533424, 31240754433, 31271244432, 31329685481, 31366881567, 31417736896, 31453450865, 31489261490, 31525124630, 31561151557, 31596932759, 31632495938, 31668126393, 31703570253, 31739247219, 31770198329, 31801284353, 31832338866, 31863245027, 31894240462, 31924997888, 31955232643, 32002059281, 32032422039, 32063047315, 32094047768, 32125274415, 32156353450, 32187458394, 32218525031, 32249732668, 32280926098, 32312245266, 32343298244, 32374436304, 32405840763, 32437302033, 32468300928, 32499438288, 32530595987, 32562028403, 32593230755, 32624616100, 32656072623, 32687115884, 32718363942, 32749514859, 32780598034, 32811095153, 32861691943, 32892698869, 32923786223, 32954563518, 32985500017, 33016805412, 33047557482, 33095799717, 33131207842, 33166594118, 33203076524, 33244878975, 33275312132, 33322658627, 33353698655, 33384992419, 33415326745, 33473253810, 33508562705, 33544119957, 33579571854, 33615365967, 33659051095, 33706759824, 33742474890, 33778213664, 33814189218, 33850123907, 33886258729, 33922252526, 33957949668, 33989350039, 34019345749, 34062147047, 34092708436, 34123201371, 34166073398, 34196374107, 34226896053, 34270017730, 34305075013, 34340209899, 34376042662, 34411775596, 34447461557, 34483274961, 34533013636, 34569312643, 34605498124, 34641578960, 34677605682, 34713731268, 34749764165, 34784141082, 34815377819, 34845903839, 34876943889, 34908282485, 34939577604, 34970802263, 35002165249, 35033553938, 35064831877, 35096312608, 35127351453, 35175337164, 35206478012, 35237657154, 35268950518, 35300414295, 35331498974, 35362737972, 35393891915, 35425098513, 35456444259, 35487618519, 35518959377, 35550039394, 35581169025, 35611385127, 35641779852, 35672327614, 35703498716, 35734702510, 35765964463, 35797225884, 35828528302, 35859796700, 35891095172, 35922431031, 35953561240, 35985006796, 36016237435, 36047412747, 36078957936, 36110290149, 36141729739, 36173044138, 36203663577, 36234770458, 36265912528, 36297251632, 36328353335, 36359085393, 36423234997, 36458913201, 36494473643, 36530249200, 36565986641, 36602360938, 36638331408, 36674229588, 36710120940, 36746047627, 36781913724, 36815060835, 36862312141, 36892540841, 36922886539, 36970737798, 37001044309, 37031633483, 37061788738, 37108978590, 37139191797, 37183315281, 37213875215, 37244461568, 37274564564, 37306011561, 37337209374, 37368070819, 37399174680, 37430039208, 37460714666, 37491446418, 37522168089, 37552905774, 37583573604, 37614330696, 37645246813, 37676133648, 37707045207, 37760591304, 37795377412, 37840042885, 37870936232, 37915105442, 37960713212, 37995418773, 38039805406, 38075037670, 38123876737, 38159330259, 38194921406, 38230609967, 38266461878, 38302066956, 38337695498, 38373263797, 38408829950, 38444438033, 38476642412, 38507718204, 38538344082, 38569296899, 38599241279, 38642032891, 38677133740, 38712731005, 38748589754, 38784472512, 38820554665, 38856384268, 38892325489, 38927421268, 38981350566, 39023299336, 39069809421, 39106207885, 39142456315, 39178920965, 39230254143, 39266250829, 39302286724, 39338329694, 39374335413, 39407154318, 39457979113, 39493815471, 39529521889, 39565074166, 39600084156, 39635505366, 39671122762, 39724328187, 39759341162, 39805354521, 39840451738, 39876529859, 39912497543, 39947451777, 39996300954, 40031341567, 40078759005, 40109448783, 40140393096, 40171264300, 40214323282, 40250088845, 40285696031, 40321426051, 40354423060, 40385446420, 40415320311, 40456282292, 40486008091, 40516318720, 40547167316, 40577883921, 40608775258, 40639178932, 40686476750, 40716618995, 40746853617, 40793300325, 40823719041, 40854226207, 40884891983, 40915984084, 40946818248, 40977930785, 41008969633, 41040058470, 41071135662, 41102269985, 41133493704, 41164572080, 41195547301, 41226629009, 41257587407, 41288214707, 41319648926, 41350867604, 41382049654, 41413388472, 41444430355, 41475580042, 41506674799, 41537787197, 41569118346, 41600333668, 41631674294, 41662688385, 41693855410, 41725243679, 41756637233, 41788045266, 41819357054, 41850393850, 41881766162, 41913230191, 41944315184, 41975481238, 42006611372, 42038049103, 42069382295, 42100678041, 42132005868, 42163417485, 42194542850, 42226075104, 42258886609, 42290408275, 42322039305, 42353400133, 42384685602, 42416034890, 42447147804, 42478416409, 42509750364, 42540994145, 42572321547, 42603094866, 42650303943, 42681477364, 42712885230, 42743266041, 42786410702, 42817623594, 42848810182, 42879219592, 42924604043, 42955670268, 42986462540, 43017403797, 43047361961, 43081324748, 43112530249, 43143462880, 43174378256, 43205337302, 43235319609, 43280391711, 43310860273, 43357713604, 43388778289, 43419156565, 43465862497, 43496558051, 43527084986, 43579956385, 43611016283, 43656529441, 43687563574, 43718801892, 43750196106, 43781415756, 43812535713, 43843232507, 43892891813, 43928343531, 43964044545, 43999711290, 44035151773, 44076765421, 44107293364, 44157256504, 44187308311, 44237775479, 44273492396, 44308986050, 44344672716, 44380129503, 44415662841, 44460770311, 44491229355, 44537907548, 44568675240, 44599139793, 44645828487, 44681572208, 44717292521, 44753000891, 44788648516, 44824327723, 44859951743, 44895054863, 44946553654, 44977458861, 45008582892, 45039514504, 45086363684, 45117298144, 45148245136, 45179060876, 45223849297, 45254758573, 45285642383, 45316710266, 45347447989, 45378455099, 45409743257, 45440686900, 45471638309, 45519803952, 45550733502, 45581436912, 45612069151, 45643028952, 45673756985, 45704563911, 45735582926, 45766315783, 45844541648, 45876017299, 45907239701, 45938330781, 45969173337, 46000174902, 46031141115, 46072943539, 46105408041, 46136478810, 46167624887, 46198402283, 46229119248, 46259317973, 46305453058, 46336426346, 46367448868, 46398449589, 46429415477, 46460545329, 46491437657, 46522259804, 46553418235, 46583343282, 46635471115, 46673622098, 46709495077, 46745337756, 46781005409, 46817027506, 46852907956, 46889124797, 46925132650, 46961096030, 46995991169, 47027133930, 47058241820, 47089350403, 47120492811, 47151481865, 47182621621, 47215466517, 47246775563, 47278067285, 47309322617, 47341547442, 47373247351, 47404602122, 47435266231, 47483300092, 47514543597, 47546089052, 47577765053, 47609066648, 47640554454, 47671782772, 47702854164, 47734200240, 47765282573, 47796680123, 47827748667, 47858519680, 47889771928, 47920836075, 47951833592, 47982792865, 48013712489, 48044926077, 48075243287, 48124780817, 48155588582, 48186444108, 48217423601, 48248411997, 48279055139, 48329549312, 48359328401, 48410799145, 48451182742, 48511661086, 48547256005, 48582802006, 48618731818, 48654621202, 48690440331, 48726192235, 48761959724, 48797837889, 48833673145, 48869728204, 48901929769, 48932734767, 48964012108, 48994791877, 49025968459, 49056838994, 49087182210, 49121522022, 49152385135, 49182976443, 49226518541, 49256403228, 49286454954, 49317167065, 49348233053, 49379080051, 49409900945, 49440831731, 49472055819, 49502984862, 49550305223, 49580364447, 49610370383, 49640589699, 49671200701, 49701658589, 49732745498, 49763705030, 49794387744, 49825377177, 49855340419, 49898148194, 49933448418, 49969208506, 50004925340, 50040476718, 50076172522, 50111991257, 50147943281, 50184100566, 50220165426, 50254740291, 50286020523, 50317241486, 50348315601, 50379329248, 50410426312, 50441860209, 50473026655, 50504182128, 50535391336, 50565661639, 50596175296, 50626820876, 50657855054, 50689120897, 50720386111, 50751478252, 50782469354, 50813493433, 50844756119, 50876141172, 50907082457, 50938264853, 50969268062, 51000442189, 51031673714, 51062803679, 51093681704, 51125022261, 51156220633, 51187370498, 51218392325, 51249629734, 51280943326, 51311324090, 51365640295, 51400833968, 51436354654, 51472084950, 51507658267, 51543367241, 51588732909, 51619782536, 51650844245, 51681448987, 51712110196, 51743294581, 51774404581, 51805613756, 51836839811, 51868190940, 51899002190, 51944479945, 51990320186, 52025678136, 52061277707, 52096944214, 52133026543, 52168829948, 52204582093, 52240328266, 52272225084, 52303303950, 52334291941, 52365438450, 52396347608, 52427114894, 52458193121, 52489087250, 52519985867, 52550749445, 52581506195, 52612555911, 52643504409, 52674555743, 52705387951, 52736504899, 52767473096, 52798288518, 52829463149, 52860482369, 52891754704, 52922876006, 52953677089, 52984576988, 53015092538, 53059216676, 53107276430, 53150683573, 53181215480, 53212038992, 53243092945, 53273777916, 53304253960, 53335128836, 53365848371, 53396748258, 53427124955, 53473248451, 53504373775, 53535291615, 53566118677, 53597002855, 53627983068, 53658721881, 53689999163, 53720849678, 53751764491, 53782694820, 53813909548, 53844931591, 53876817385, 53907815675, 53938953969, 53969946218, 54001086472, 54032251037, 54063232103, 54094150942, 54125057897, 54156191875, 54187425946, 54218311903, 54249453853, 54280418521, 54311524607, 54342474927, 54373203339, 54404624936, 54436936482, 54468047288, 54499540862, 54530496556, 54561576645, 54592531858, 54623615384, 54666956730, 54706869266, 54741055782, 54772209863, 54803337769, 54853908317, 54884931808, 54915857148, 54946877878, 54977853707, 55008789506, 55039680724, 55070510952, 55101311023, 55132033430, 55162850043, 55193946986, 55225054340, 55255276915, 55307286401, 55351188377, 55400037797, 55430450091, 55461027981, 55491157981, 55539620389, 55574942709, 55610500563, 55646211810, 55682390649, 55718422810, 55754793097, 55791027418, 55836334548, 55867835639, 55898579478, 55929701760, 55960649034, 55991622795, 56022385087, 56053357089, 56084296276, 56115204600, 56164426162, 56200428364, 56236560871, 56272547067, 56308539481, 56344603156, 56376705594, 56407776268, 56438924680, 56470133012, 56501273727, 56532816088, 56564067116, 56595067246, 56626184930, 56657229726, 56688361358, 56720978489, 56752798185, 56783301362, 56827962894, 56863543261, 56905236613, 56936323170, 56967657956, 56998757808, 57029953518, 57061034773, 57092177019, 57123017442, 57167299540, 57218980579, 57269306328, 57304909900, 57340483467, 57376128392, 57411693398, 57447436703, 57482990991, 57518738766, 57554831842, 57590545668, 57626246307, 57658353538, 57689272953, 57719268020, 57766611421, 57797622138, 57828773861, 57859930059, 57891301875, 57922323432, 57953398928, 57984458527, 58015526131, 58046968033, 58096989685, 58128031697, 58159170550, 58190278911, 58221455399, 58252786603, 58283258329, 58333816687, 58369893750, 58406238690, 58442456044, 58478805914, 58514730290, 58550968146, 58599202771, 58629910557, 58660792078, 58691406282, 58743469920, 58779690198, 58816580135, 58854960696, 58903753561, 58944524789, 58976782160, 59008524059, 59042441557, 59074570911, 59106403210, 59137869558, 59168967786, 59199950181, 59231057290, 59262160740, 59293211443, 59324389981, 59355588753, 59386402498, 59417274282, 59447401361, 59494379702, 59534162286, 59564361257, 59594832024, 59625819078, 59656926584, 59687248693, 59735215320, 59784216097, 59819425162, 59868344066, 59899368367, 59930202240, 59960995993, 59991667280, 60022220398, 60053661522, 60084584498, 60115084953, 60163969657, 60194432237, 60225055335, 60255309424, 60303088742, 60345834897, 60381959943, 60418002717, 60454010784, 60490002233, 60526189508, 60562370490, 60598397348, 60634550872, 60670546233, 60706593598, 60739005585, 60770402548, 60801239526, 60831302230, 60888372039, 60919133451, 60966546202, 60997375239, 61028288640, 61059193516, 61090011178, 61121041494, 61152194900, 61182915338, 61213695581, 61244636997, 61275208872, 61320126221, 61367634153, 61406199410, 61437409716, 61468712871, 61499146746, 61551888625, 61583033614, 61629186130, 61660319669, 61691183925, 61723253257, 61755253754, 61803121745, 61851097196, 61898021390, 61928875987, 61960182566, 61992138122, 62026601485, 62059303214, 62105106121, 62143649763, 62175060396, 62206487979, 62237821269, 62268828307, 62299739169, 62345915128, 62376225404, 62407019330, 62452720793, 62483217395, 62524025841, 62554652447, 62585206002, 62615212593, 62658833687, 62698264339, 62728140836, 62758845886, 62789656566, 62820634549, 62851494688, 62882577172, 62913645471, 62945253799, 62976761189, 63008623197, 63040370721, 63072456260, 63105121550, 63137122315, 63168629803, 63199819932, 63230756463, 63261922138, 63292844781, 63323288462, 63370734567, 63401802519, 63432990166, 63463242262, 63511981149, 63547002085, 63601297766, 63631349828, 63673957602, 63705245282, 63735363101, 63781314600, 63811361954, 63854871806, 63896864116, 63932373633, 63968386652, 64004750402, 64040853365, 64077330489, 64113894481, 64150439734, 64185840611, 64221880332, 64255680647, 64286603428, 64317908670, 64349591561, 64381008393, 64413695382, 64445296429, 64477116119, 64508877318, 64540566771, 64572127103, 64603225355, 64659347794, 64690318248, 64721369555, 64752617557, 64783131971, 64827535079, 64858144123, 64889448920, 64920652020, 64951774032, 64982821321, 65013894055, 65045275464, 65076843650, 65108121931, 65139592969, 65170867549, 65201995042, 65233375975, 65265257809, 65296625704, 65328159343, 65360028629, 65392021495, 65424535137, 65456402389, 65489983867, 65522927696, 65555035274, 65595085495, 65632188616, 65667907693, 65703563511, 65739553195, 65781827738, 65823960420, 65860097437, 65896331600, 65932416977, 65968474395, 66005008263, 66040305686, 66075270575, 66116252284, 66149177523, 66179981934, 66210991225, 66260502570, 66295805478, 66331270743, 66366666347, 66401917614, 66437222080, 66472659548, 66508223296, 66540187118, 66571161917, 66614026470, 66657878518, 66693923941, 66746901640, 66797298519, 66834016510, 66870469328, 66906602409, 66942636275, 66978740460, 67014711014, 67050774982, 67086917098, 67145095756, 67181425912, 67219234585, 67257233930, 67293296527, 67329523058, 67362043137, 67393749856, 74654001131]\n",
      "Aggregating data over 1876 iterations: iteration 0 start (0 ns) to iteration 1875 end (74654001131 ns)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DLProf-09:59:33] Aggregating profile data\n",
      "[DLProf-09:59:38] Creating dlprof database at ./dlprof_dldb.sqlite\n",
      "[DLProf-09:59:38] Writing profile data to dlprof database\n",
      "[DLProf-09:59:48] Writing aggregated data to dlprof database\n",
      "[DLProf-10:00:03] Writing expert_systems report to (stdout)\n",
      "Expert Systems Feedback: 5 issues detected. Note that expert systems is still experimental as are all recommended changes\n",
      "\n",
      "Problem detected: \n",
      "  945 ops were eligible to use tensor cores but none are using FP16\n",
      "Recommended change: \n",
      "  Try enabling AMP (Automatic Mixed Precision). For more information: https://developer.nvidia.com/automatic-mixed-precision\n",
      "\n",
      "Problem detected: \n",
      "  The GPU is underutilized: Only 0.6% of the profiled time is spent on GPU kernel operations\n",
      "Recommended change: \n",
      "  Dataloader has the highest (non-GPU) usage at 67.5%. Investigate the dataloading pipeline as this often indicates too much time is being spent here\n",
      "\n",
      "Problem detected: \n",
      "  67.5% of the aggregated run was spent in the dataloader while not simultaneously running on the GPU\n",
      "Recommended change: \n",
      "  Focus on reducing time spent in the training data input process. This could be time spent in file reading, preprocessing and augmentation or file transfer.\n",
      "Set num_workers > 0 in the dataloader to enable asynchronous data loading.\n",
      "Set pin_memory = True in the dataloader to enable faster memcpy operations.\n",
      "Consider using NVIDIA DALI, a library that is a high performance alternative to built-in data loaders and data iterators. Learn more here: https://developer.nvidia.com/DALI\n",
      "\n",
      "Problem detected: \n",
      "  The aggregated iteration range of 0 to 1875 contains a lot of variation\n",
      "Recommended change: \n",
      "  Try limiting the iteration range to a steady range by rerunning with the --database option and setting --iter_start=362 --iter_stop=423\n",
      "\n",
      "Problem detected: \n",
      "  GPU Memory is underutilized: Only 9% of GPU Memory is used\n",
      "Recommended change: \n",
      "  Try increasing batch size by 4x to increase data throughput\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Profile the required Python script saved as .py file\n",
    "\n",
    "!dlprof --force=true --mode='pytorch' python3 /home/user/1_VIDHYA/DLPROF/DEMO/train.py\n",
    "# --force=true - to overwrite the previous event files\n",
    "# train.py is same as the notebook file we created in Pytorch Catalyst with few modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dlprofviewer-10:02:20 AM UTC] dlprofviewer running at http://localhost:8000\n",
      "^C\n",
      "[2022-02-20 15:33:22 +0530] [10876] [WARNING] Worker with pid 10879 was terminated due to signal 3\n"
     ]
    }
   ],
   "source": [
    "# View the Rsults\n",
    "!dlprofviewer dlprof_dldb.sqlite\n",
    "# dlprof_dldb.sqlite - Filename - check with ls command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for user: \n"
     ]
    }
   ],
   "source": [
    "# METHOD2 - Have to use command line Terminal\n",
    "\n",
    "# Docker prevents problems related to installation of Deep Learning environment. \n",
    "# No more fighting with Cuda versions and GCC compilers. \n",
    "# Welcome to the docker era. All you need is a Unix system with updated Cuda drivers. \n",
    "# And you need is to download a docker from NGC cloud that contains the environment you need with all the Cuda code.\n",
    "\n",
    "# Steps:\n",
    "\n",
    "#0 Ensure you have access and are logged into NGC\n",
    "# Create Account if new user from REF4\n",
    "\n",
    "#1 Docker Engine \n",
    "# docker -v Check if Docker already installed and its version \n",
    "# Docker version 20.10.11, build dea9396\n",
    "# If not, install docker\n",
    "\n",
    "#2 Nvidia Docker - NVIDIA Container Toolkit\n",
    "# nvidia-docker version Check if Nvidia Docker already installed and its version\n",
    "# Not Installed\n",
    "# Follow the Steps ##1 to ##3 to install nvidia-docker\n",
    "##1 NVIDIA Driver\n",
    "# cat /proc/driver/nvidia/version - use this command to check if CUDA Driver installed and its version\n",
    "# nvidia-smi (system management interface) - check nvidia driver version\n",
    "##2 Check platform requirements in REF3\n",
    "##3 Install Nvidia Docker - REF5 - Below are the commands \n",
    "\n",
    "!distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \\\n",
    "   && curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - \\\n",
    "   && curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n",
    "# Most Linux package managers have the ability to validate the itegrity of a software package before installation by \n",
    "# verifying itâ€™s PGP(GPG) key\n",
    "# GPG Key - GNU Privacy Guard - GNU is a recursive acronym for \"GNU's Not Unix!\"\n",
    "        \n",
    "!sudo apt-get update \n",
    "!sudo apt-get install -y nvidia-docker2\n",
    "# Install the nvidia-docker2 package (and dependencies) after updating the package listing:\n",
    "\n",
    "!sudo systemctl restart docker\n",
    "# Restart docker to complete installation\n",
    "\n",
    "!sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi\n",
    "!nvidia-docker version\n",
    "# A working setup can be tested by running a base CUDA container\n",
    "# --rm option means automatically remove the container when it exits.\n",
    "# --gpus all is used to assign all available gpus to the docker container.\n",
    "# nvidia/cuda:11.0-base - Includes the CUDA runtime (cudart). It does not include CUDA math libraries etc.\n",
    "  \n",
    "#REF1: https://docs.nvidia.com/deeplearning/frameworks/dlprof-user-guide/#quickstart_topic\n",
    "#REF2: https://github.com/NVIDIA/nvidia-docker\n",
    "#REF3: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker\n",
    "#REF4: https://ngc.nvidia.com/signin\n",
    "#REF5: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the NGC Docker Container\n",
    "\n",
    "!docker login nvcr.io\n",
    "# username - $oauthtoken\n",
    "# password - API Key from the NGC Account\n",
    "# Login Successful\n",
    "\n",
    "!docker pull nvcr.io/nvidia/pytorch:21.02-py3 \n",
    "# Pull the required Container\n",
    "\n",
    "!docker images \n",
    "# View the pulled image\n",
    "\n",
    "# REF1: https://docs.nvidia.com/deeplearning/frameworks/user-guide/index.html#pullcontainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for user: \n"
     ]
    }
   ],
   "source": [
    "# Launching the NGC Container\n",
    "\n",
    "!docker run --gpus all -it --rm --network=host -v /home/user/1_VIDHYA/DLPROF/DEMO:/workspace/DEMO nvcr.io/nvidia/pytorch:21.02-py3\n",
    "# The docker run command creates a container from a given image and starts the container using a given command.\n",
    "# -it runs Docker interactively\n",
    "# --network=<network-name> option to start a container and immediately connect it to a network.\n",
    "# -v flag mounts the current working directory into the container.\n",
    "\n",
    "# We enter the prompt - root@userpc:/workspace#\n",
    "# Installing necessary packages \n",
    "# DLPROF version - 1.8.0\n",
    "\n",
    "pip install nvidia-pyindex \n",
    "# This package adds the address of the NVIDIA Private Python Package Index(repository of Software)to the user's environment\n",
    "# install the NVIDIA PY index\n",
    "\n",
    "pip install nvidia-dlprof[pytorch]\n",
    "# installs the nvidia-pytorch pip package from the NVIDIA PY index and dlprofâ€™s python pip package for pytorch models\n",
    "# !pip install nvidia_dlprof_pytorch_nvtx - Not needed since exist in prev install\n",
    "# Pytorch does not have built in NVTX ranges around operations. \n",
    "# As such, DLProf has to rely on a python pip package called nvidia_dlprof_pytorch_nvtx \n",
    "# The NVIDIA Tools Extension Library (NVTX) - Applications which integrate NVTX can use NVIDIA Nsight VSE to capture and \n",
    "# visualize these events and ranges.\n",
    "\n",
    "pip install nvidia-dlprofviewer\n",
    "# To view the results\n",
    "\n",
    "dlprof --force=true --mode='pytorch' python /workspace/DEMO/train.py\n",
    "# --force=true - to overwrite the previous event files\n",
    "# train.py is same as the notebook file we created in Pytorch Catalyst with few modifications\n",
    "\n",
    "tensorboard --port 8000 --logdir=event_files # We will get a link which can be pasted in browser and the profiling details can be viewed.\n",
    "# View the Results in Tensorboard - Sometimes Error\n",
    "\n",
    "#OR\n",
    "\n",
    "dlprofviewer dlprof_dldb.sqlite\n",
    "# View the Results in DLPROFViewer\n",
    "# dlprof_dldb.sqlite - Filename - check with ls command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the same with train_amp.py - Only change in the .py code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently Working\n",
    "dlprof --force=true --mode='pytorch' python /workspace/DEMO/train.py --data-backends dali-gpu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
